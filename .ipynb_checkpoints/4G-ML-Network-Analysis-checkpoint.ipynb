{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4987e0e-c158-4699-bc99-b9feaeaae1a2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 24px; margin-bottom: 50px;\">  \n",
    "    <strong>4G ML Network Analysis Notebook </strong> \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center; font-size: 18px; margin-bottom: 50px;\">\n",
    "    Prepared by:  Onur Tuncay   \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3af1fc-a326-46c7-98b3-c86d3a917dad",
   "metadata": {},
   "source": [
    "### Nobebook Structure\n",
    "This notebook is organized to provide a step-by-step exploration of a 4G mobile network dataset, applying data science and machine learning methodologies across multiple tasks. The structure is aligned with the designated tasks and aims to ensure clarity and coherence throughout the analysis:\n",
    "\n",
    "- Section 1 introduces the motivation and rationale for selecting the dataset, with a focus on its relevance and suitability for real-world network analysis.\n",
    "\n",
    "- Section 2 offers an overview of the selected dataset, highlighting its features, structure, and variables that will be used throughout the analysis.\n",
    "\n",
    "- Section 3 (Task 1) covers data preprocessing and exploration, including initial data summaries, handling of categorical and temporal columns, treatment of missing values, outlier detection, and data normalization. This section also presents various visualizations to uncover patterns within the dataset.\n",
    "\n",
    " - Section 4 (Task 2) is dedicated to clustering analysis, where both K-Means and Agglomerative clustering techniques are applied. The performance of clustering algorithms is compared, and clusters are interpreted using domain-relevant variables.\n",
    "\n",
    " - Section 5 (Task 3) addresses classification tasks, involving both binary and multi-class classification. Several models are trained and evaluated using metrics such as accuracy and F1-score, supported by feature importance and SHAP analysis for interpretability.\n",
    "\n",
    " - Section 6 (Task 4) demonstrates the use of Genetic Algorithms for hyperparameter optimization, focusing on improving the performance of a Random Forest classifier through evolutionary computation principles.\n",
    "\n",
    " - Section 7 provides references for this notebook.\n",
    "\n",
    "This structure ensures a comprehensive analysis framework that integrates exploratory data analysis, unsupervised learning, supervised classification, and model optimization. It provides a data-driven perspective on mobile network behavior and supports data-informed decision-making for performance enhancement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7e626-3d45-4cb6-a6d9-61a95b477343",
   "metadata": {},
   "source": [
    "<a id=\"1-introduction\"></a> \n",
    "# 1. Dataset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae3f99-e5a1-4409-84e5-9dd96df214de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd  # Data manipulation and analysis using DataFrames\n",
    "import glob  # File handling and retrieval using patterns\n",
    "import numpy as np  # Numerical computing, including arrays and mathematical operations\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "import matplotlib.pyplot as plt  # Fundamental plotting library\n",
    "\n",
    "# Import preprocessing utilities\n",
    "from sklearn.preprocessing import StandardScaler  # Standardizing features (zero mean, unit variance)\n",
    "from sklearn.preprocessing import LabelEncoder  # Encoding categorical variables into numerical format\n",
    "\n",
    "# Import statistical functions\n",
    "from scipy.stats import zscore  # Compute Z-score for outlier detection\n",
    "from scipy.stats import chi2_contingency  # Perform Chi-square test for categorical independence\n",
    "\n",
    "# 3D visualization tools\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Enables 3D plotting\n",
    "\n",
    "# Clustering algorithms\n",
    "from sklearn.cluster import KMeans  # K-Means clustering for unsupervised learning\n",
    "from sklearn.metrics import silhouette_score  # Evaluate clustering quality with silhouette score\n",
    "from sklearn.decomposition import PCA  # Dimensionality reduction via Principal Component Analysis\n",
    "from sklearn.cluster import AgglomerativeClustering  # Hierarchical clustering\n",
    "\n",
    "# Hierarchical clustering visualization\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  # Create dendrograms for hierarchical clustering\n",
    "\n",
    "# Explainability tools\n",
    "import shap  # SHAP values for feature importance explanation in machine learning models\n",
    "\n",
    "# Train-test split utility\n",
    "from sklearn.model_selection import train_test_split  # Splits data into training and testing sets\n",
    "from sklearn.model_selection import cross_val_score  # Cross-validation \n",
    "\n",
    "# Classification models\n",
    "from sklearn.ensemble import RandomForestClassifier  # Ensemble tree-based classifier\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression for binary classification\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score  # Compute classification accuracy\n",
    "from sklearn.metrics import confusion_matrix  # Generate confusion matrix for model evaluation\n",
    "from sklearn.metrics import classification_report  # Summarize precision, recall, F1-score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, make_scorer  # Individual classification metrics\n",
    "\n",
    "import random  # Random number generation for parameter selection\n",
    "import scipy.stats as stats  # Statistical functions like z-score\n",
    "\n",
    "\n",
    "# Suppress warnings for readability\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore unnecessary warnings\n",
    "\n",
    "# Configure Pandas display settings\n",
    "pd.set_option('display.max_rows', 2000)  # Show up to 2000 rows in DataFrame outputs\n",
    "pd.set_option('display.max_columns', 500)  # Show up to 500 columns in DataFrame outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88820da-36d6-46f3-aedc-e215fe301a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd  #Check current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc209287-537b-4077-88dc-6f65778125cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path of the datasets\n",
    "file_path = \"C:\\\\Users\\\\onurt\\\\OneDrive\\\\Desktop\\\\UoG Msc Data Science\\\\CT7205 - Machine Learning and Optimisation\\\\Assessment\\\\dataset/*.csv\"\n",
    "\n",
    "# An empty dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through all CSV files\n",
    "for file in glob.glob(file_path):\n",
    "    # Extract the file name and use it as a key\n",
    "    file_name = file.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    # Read the CSV file into a DataFrame and replace \"?\" values with NaN\n",
    "    df = pd.read_csv(file, index_col=0, na_values=\"?\")\n",
    "    # Save the DataFrame in the dictionary\n",
    "    dataframes[file_name] = df\n",
    "    \n",
    "    # File name\n",
    "    print(f\"File: {file_name}\")\n",
    "    # Shape and missing value analysis\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "    print(\"\\nPercentage of Missing Values:\\n\", (df.isnull().sum() / len(df) * 100).round(2))\n",
    "    \n",
    "    # General information about the DataFrame\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    df.info()\n",
    "    \n",
    "    # Summary statistics of the DataFrame\n",
    "    print(\"\\n--- Summary Statistics ---\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f4461-fd31-4111-ad9e-ca9e11b2f63b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The selection of a correct dataset is critical to ensure correct analysis and relevant insights. Among the provided datasets, the 4G - Passive Measurements dataset was chosen for this study since it offers a less missing values and less records which can be useful for efficient analysis and modelling process. Another reason for selecting this dataset is that it is relatively small in comparison with the 5G dataset and thus more manageable in terms of processing through the current computational resources. The selected 4G dataset consists of 527,540 records and it includes both numerical and categorical 26 variables. This combination offers a comprehensive representation of various network conditions. Notably, the missing data values are lower than the other datasets, especially for critical features like signal strength indicators and mobility metrics. On the other hand, this ensures that preprocessing can be conducted efficiently with minimal loss of information (Hossain and Inoue, 2019). </div>   \n",
    "<br >\n",
    "<div style=\"text-align: justify;\">\n",
    "However, the 5G - Passive Measurements dataset contains over 8 million records, which would require a significant amount of computational resources and memory allocation. In additon to this, over 60 variables in the 5G dataset have more than 90% missing values. On the other hand, Latency Tests - Online Gaming and Throughput Tests - Speedtest datasets contain an excessive amount of missing values, which makes them less suitable to perform detailed and reliable analyses. Additionally, the problem of missing values is common in data-driven fields and can often result in decreased model performance, compromising statistical inference. It also leads to biased estimates due to fundamental discrepancies between observed and missing data distributions (Ayilara et al., 2019). Moreover, the NB-IoT - Passive Measurements dataset, though suitable, is less in size and more low-power wide-area network-centric, hence it is less suitable for the objectives of this study.\n",
    "By focusing on the 4G dataset, this study aims to explore the network performance for various scenarios in terms of varying signal power, mobility scenarios, and atmospheric conditions with ensuring efficiency in the computational area.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d0ba7-d4e2-4d42-bedc-09239539b35f",
   "metadata": {},
   "source": [
    "<a id=\"2-dataset-overview\"></a> \n",
    "# 2. Selected Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d154b-aff8-44fc-a686-28d2a584f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"4G - Passive measurements.csv\", index_col=0)  # Read the selected dataset with considering unnamed and irrelevant column with adding index_col parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697bc611-8e5b-444f-bbae-b0f9c6952aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f9d0872-2a59-4756-8823-3d09a09cdf2c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The dataset consists of 527,540 records and 26 features covering a wide range of network parameters. This dataset shared by  Kousias et al. (2024). They provide a comprehensive dataset of network measurements, covering 4G. Besides, the data was collected over a seven-week period in Rome, Italy, covering a diverse range of urban environments and mobility scenarios. Measurement campaigns were conducted on the infrastructures of two major mobile network operators (MNOs), allowing for the inclusion of heterogeneity in network deployment and end-user experience. Moreover, the table given below indicates  features and their descriptions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc984c94-f002-44f1-bfcd-3ad56d09ef09",
   "metadata": {},
   "source": [
    "| Feature Name               | Description                                                                 | Data Type   |\n",
    "|---------------------------|-----------------------------------------------------------------------------|-------------|\n",
    "| Date, Time                | Timestamp of the measurement                                                | Temporal    |\n",
    "| UTC                       | Coordinated Universal Time in Unix format                                   | Numerical   |\n",
    "| Latitude, Longitude, Altitude | UE (User Equipment) GPS coordinates [in °]                              | Numerical   |\n",
    "| cellLatitude, cellLongitude | Cell tower GPS coordinates [in °]                                        | Numerical   |\n",
    "| cellPosErrorLambda1, cellPosErrorLambda2 | Estimated error in cell positioning [in meters]                   | Numerical   |\n",
    "| Speed                     | Movement speed of the UE [km/h]                                             | Numerical   |\n",
    "| Frequency, EARFCN         | Carrier frequency and E-UTRA Absolute RF Channel Number                     | Numerical   |\n",
    "| MNC                       | Mobile Network Code (operator identifier)                                   | Categorical |\n",
    "| PCI                       | Physical Cell Identifier                                                   | Numerical   |\n",
    "| CellIdentity              | Cell ID                                                                    | Numerical   |\n",
    "| eNodeB.ID                 | Evolved Node B Identifier (base station ID)                                | Numerical   |\n",
    "| RSRP                      | Reference Signal Received Power [dBm]                                      | Numerical   |\n",
    "| RSRQ                      | Reference Signal Received Quality [dB]                                     | Numerical   |\n",
    "| SINR                      | Signal to Interference and Noise Ratio [dB]                                | Numerical   |\n",
    "| Power                     | Received power level [dBm]                                                 | Numerical   |\n",
    "| n_CellIdentities          | Number of detected Cell IDs per eNodeB                                     | Numerical   |\n",
    "| distance                  | Line-of-sight distance from the cell [m]                                   | Numerical   |\n",
    "| Band                      | Radio frequency band identifier                                           | Numerical   |\n",
    "| scenario                  | Environment classification: Indoor, Outdoor, Vehicle                       | Categorical |\n",
    "| campaign                  | Campaign ID associated with the measurement                                | Categorical |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de7a93-6db2-4a25-a1e2-103fc0c2a8ab",
   "metadata": {},
   "source": [
    "<a id=\"3-data-preprocessing--exploration\"></a> \n",
    "# 3. Data Preprocessing & Exploration (Task 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a4c5d3-b266-479a-b63f-1c4a8d38da75",
   "metadata": {},
   "source": [
    "#### Duplicate Records Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d28e07e-b6fd-4d88-807d-08bd38f6a262",
   "metadata": {},
   "source": [
    "Initially, a duplicate record check was performed on the dataset, and no duplicate records were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289fe0bd-786c-4cf8-8ddd-fe835a12e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(keep=False)]\n",
    "\n",
    "# Print the number of duplicate rows\n",
    "print(f\"Number of duplicate rows: {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bcc5ed-f230-4580-a196-c60c8eb2a07c",
   "metadata": {},
   "source": [
    "<a id=\"31-handling-categorical-date-columns\"></a> \n",
    "### 3.1.  Basic Dataset Summary & Handling Non-numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a23f29-e880-4db0-bdf5-f470b976b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about 4G Dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\\n\", df.info())\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "print(\"\\nSummary Statistics:\\n\", df.describe())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ff4c0c0-d7f7-432e-bcaf-d1443efc6782",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "Key signal quality metrics such as RSRP, RSRQ, SINR, and Power are included, alongside spatial indicators like Latitude, Longitude, and Altitude. Network-specific features such as EARFCN, PCI, and CellIdentity are also present. Some variables such as UTC, Altitude, Speed, Power, SINR, and cellPosErrorLambda1 contain missing values, but the majority of features are complete. Summary statistics show that signal strength values (e.g., RSRP and Power) vary significantly across the dataset. The Speed ranges from stationary to over 79 km/h, indicating varying user mobility. Additionally, the values for Frequency and Band suggest that the dataset captures measurements across multiple radio frequencies and cellular bands. These diverse features make the dataset well-suited for both classification and clustering tasks. \r\n",
    "</divs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1ed80fa-9510-41ee-9d56-b824d1d77f7d",
   "metadata": {},
   "source": [
    "Handling Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfd3ee-2433-4ce9-902f-5c4fb0d7d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns could be considered as a categorical\n",
    "cols_to_check = [\"EARFCN\",\"MNC\", \"scenario\", \"campaign\",\"PCI\", \"eNodeB.ID\", \"CellIdentity\"]  \n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(f\"Number of Unique values for {col}:\")\n",
    "    print(df[col].nunique())\n",
    "    print(f\"Top 20 most frequent values for {col}:\")\n",
    "    print(df[col].value_counts(normalize=True).cumsum().head(20).to_frame().reset_index())\n",
    "    #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b55783-3282-4146-8375-92d70c383992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns datatype conversion\n",
    "categorical_cols = [\"EARFCN\",\"MNC\", \"scenario\", \"campaign\",\"PCI\", \"eNodeB.ID\", \"CellIdentity\"]\n",
    "df[categorical_cols] = df[categorical_cols].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcaaeef-8a29-4667-9fb5-6ff106f0cc67",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "In this step, several columns were analyzed to determine whether they should be treated as categorical variables. The columns selected for inspection included EARFCN, MNC, scenario, campaign, PCI, eNodeB.ID, and CellIdentity. They represent discrete identifiers or classifications. For each column, the number of unique values and the cumulative frequency of the most common values were examined. Based on this analysis, all listed columns were converted to the categorical data type. This conversion is crucial for improving memory efficiency and ensuring appropriate treatment of these variables in subsequent modeling tasks.\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3205b69-00cc-4b2d-931e-77b83fd01626",
   "metadata": {},
   "source": [
    "#### Handling Time Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ed70a-9051-411e-bf68-a31e30b8b290",
   "metadata": {},
   "source": [
    "\n",
    "To streamline temporal processing and enhance interpretability, Date and Time columns were merged into a single DateTime column. This transformation ensured that all timestamp-related information was consolidated in one place, simplifying time-based operations and enabling easier extraction of temporal patterns for subsequent analysis.  \n",
    "    \n",
    "Tolidate consistency across time representations, the newly created DateTime column was compared with the existing UTC (Unix Time) column. The difference between the two columns was consistently 30 days for all records. Since both columns effectively carried the same information, the UTC column was dropped to avoid redundancy and potential multicollinearity issues during modeling.\n",
    "odeling.\n",
    "\n",
    "To further enrich the dataset, a series of new time-related features were engineered from the DateTime column:\n",
    "\n",
    "- minute\n",
    "\n",
    "- hour\n",
    "\n",
    "- day\n",
    "\n",
    "- month\n",
    "\n",
    "- day_of_week\n",
    "\n",
    "- is_weeken\n",
    "\n",
    "\n",
    "- year\n",
    "\n",
    "These features were created to capture temporal dynamics that may influence network behavior, such as diurnal usage patterns or weekday/weekend effects. According to Forke and Tropmann-Frick (2021), deriving such features is a crucial aspect of the feature engineering process, especially when raw data lacks semantic  clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff50ddde-3c88-4297-98ed-44e2d1b91037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], errors='coerce') # creating a new DateTime columns in datetime format\n",
    "df.drop(['Date', 'Time'], axis=1, inplace=True) ## Since Date and Time information stored in new DateTime columns there no need to keep the old columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877ef57-597f-46ba-b878-33f45c3bb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"UTC\"] = pd.to_datetime(df['UTC'], unit='s') ## converting UTC to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fc717-ef63-4ece-99f8-3690cd02a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"UTC\",\"DateTime\"]].head(10) ##investigatiing UTC and DateTime Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c886146-bcb7-45e6-a71b-af8044bfdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DateDiff\"]= df[\"UTC\"]-df[\"DateTime\"] ##creating a new columns which stores time diff between UTC and DateTime columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7972c349-0103-4707-b169-456453f1c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DateDiff\"].dt.days.value_counts().to_frame().reset_index()  ## investigating the unique day diffs and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7aecf-363a-4fb6-a63f-47d1be77f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by unique day differences and counting occurrences\n",
    "day_counts = df[\"DateDiff\"].dt.days.value_counts().sort_index()\n",
    "\n",
    "# Creating Bar Chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=day_counts.index, y=day_counts.values, color=\"skyblue\")\n",
    "\n",
    "# Adding text annotations on bars\n",
    "for index, value in enumerate(day_counts.values):\n",
    "    plt.text(index, value + 0.5, str(value), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Adjusting titles and labels\n",
    "plt.title(\"Frequency of Day Differences (between UTC and DateTime Columns)\", fontsize=14)\n",
    "plt.xlabel(\"Day Difference\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869f0d8-e149-46d2-bace-79b51bea36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new time columns\n",
    "df[\"minute\"] = df[\"DateTime\"].dt.minute\n",
    "df[\"hour\"] = df[\"DateTime\"].dt.hour                \n",
    "df[\"day\"] = df[\"DateTime\"].dt.day                  \n",
    "df[\"month\"] = df[\"DateTime\"].dt.month              \n",
    "df[\"day_of_week\"] = df[\"DateTime\"].dt.dayofweek    \n",
    "df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "df[\"year\"] = df[\"DateTime\"].dt.year\n",
    "df[\"is_weekend\"] = df[\"is_weekend\"].astype('category') #converting is_weekend to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6aa87c-9061-4624-865f-bd04eb0d8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"DateDiff\", \"UTC\"], inplace = True) ## Since there is a constant time diff between UTC and DateTime there is no need to keep both of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387ad0c-877d-4843-a069-87ad5cd2a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"DateTime\",\"minute\",\"hour\",\"day\",\"month\",\"day_of_week\",\"is_weekend\",\"year\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd12b43-e2ab-4ced-a746-e34def934a66",
   "metadata": {},
   "source": [
    "<a id=\"32-missing-values\"></a> \n",
    "### 3.2. Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4b818-56aa-4b98-95d3-68a441b94aac",
   "metadata": {},
   "source": [
    "In the preprocessing phase of this notebook, special attention was paid to handling missing data in a dataset containing 4G passive measurements and a group-based handling missing values approach was implemented based on domain knowledge and exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc251039-2a56-4bab-812f-e32049c970ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "total_rows = len(df)\n",
    "missing_percent = (missing_counts / total_rows) * 100\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12,6))  \n",
    "ax = sns.barplot(x=missing_counts.index, y=missing_counts.values, palette=\"viridis\")\n",
    "\n",
    "# Add percentages above the bars a\n",
    "for i, p in enumerate(ax.patches):\n",
    "    if missing_counts[i] > 0:  # Only add labels for columns with missing values\n",
    "        ax.annotate(f'{missing_percent[i]:.2f}%', \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha='center', va='bottom', fontsize=6, color='black', fontweight='bold')  \n",
    "\n",
    "# Adjust axis\n",
    "plt.xticks(rotation=30, ha='right', fontsize=9)  # Rotate and enlarge x-axis labels\n",
    "plt.ylabel(\"Count of Missing Values\", fontsize=9)\n",
    "plt.title(\"Distribution and Percentage of Missing Values\", fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5476ef-cc3f-405f-9933-ea203dbcc381",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r\n",
    "As seen in figure above, missing data are included in the columns Speed, Altitude, Power, SINR, and cellPosErrorLambda1. Speed and Altitude had about 13.3% missing value rates, while Power and SINR showed about 4.12% and cellPosErrorLambda1 showed about 1.5% missing records. In addition, the heat map of missing values in figure below reveals patterns that indicate that missing values are not completely random. Therefore, it can be argued that a context-aware strategy would be more appropriate\n",
    "</div>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088ce79-8df4-4c04-b5ba-39d36e844426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for the distribution of missing values\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(df.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n",
    "plt.title(\"Distribution of Missing Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac274c3-a65e-4227-b238-284c7b779de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e498a8a-cbab-4be2-ab75-877cf880369a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "In terms of filling missing values, grouped median imputation was applied to preserve the spatial and signal integrity of the data. In detail, Altitude and Speed were filled using the median value in each scenario category. The scenarios represent different collection contexts that may affect the expected range of values of these measurements. Group-based imputation ensures that the filled values reflect the environmental conditions of the original data collection. Power and SINR were filled according to the groupings made by CellIdentity and EARFCN (frequency channel). These features are tightly linked to signal transmission characteristics and cell tower configurations. Estimation within these groups helps maintain network consistency and more accurately reflect real-world radio conditions. Furthermore, cellPosErrorLambda1 was filled using the global median due to its relatively low missing rate and lack of strong grouping relationships. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2ade2-858a-4f2c-a2ad-d367d55c6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values for Altitude and Speed with the overall median\n",
    "df['Altitude'] = df.groupby('scenario')['Altitude'].transform(lambda x: x.fillna(x.dropna().median()) if not x.dropna().empty else df['Altitude'].median())\n",
    "df['Speed'] = df.groupby('scenario')['Speed'].transform(lambda x: x.fillna(x.dropna().median()) if not x.dropna().empty else df['Speed'].median())\n",
    "\n",
    "\n",
    "# Filling missing values for Power and SINR based on Cell Identity and Frequency grouping\n",
    "df['Power'] = df.groupby(['CellIdentity', 'EARFCN'])['Power'].transform(lambda x: x.fillna(x.dropna().median()) if not x.dropna().empty else df['Power'].median())\n",
    "df['SINR'] = df.groupby(['CellIdentity', 'EARFCN'])['SINR'].transform(lambda x: x.fillna(x.dropna().median()) if not x.dropna().empty else df['SINR'].median())\n",
    "\n",
    "# Filling missing values for cellPosErrorLambda1 based on median\n",
    "df['cellPosErrorLambda1'].fillna(df['cellPosErrorLambda1'].median(), inplace=True)\n",
    "\n",
    "# Final checks  if any missing values remain\n",
    "print(df.isnull().sum())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df866a84-8e39-4f19-b926-8bd4fbf5f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for columns with missing values\n",
    "missing_cols = ['Altitude', 'Speed', 'Power', 'SINR', 'cellPosErrorLambda1']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(missing_cols, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(df[col], bins=50, kde=True)\n",
    "    plt.title(f'{col} Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae5496-17f1-4c26-bc9c-c121d3e126bb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r\n",
    "\n",
    "The figureabovew shows that the central tendencies and spread of the imputed features remain consistent with realistic network behaviour. For example, Altitude and Speed maintain reasonable distributions consistent with expected physical motion in real-world scenarios. Furthermore, Power and SINR exhibit typical bell-shaped and multimodal distributions, respectively, consistent with known signal quality profiles in cellular networks. All in all, this strategy not only ensures the completeness of the dataset but also increases its representativeness for subsequent clustering and classification tasks\n",
    "</div>\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0660f-cc0e-4ef2-9190-6fa9933700c7",
   "metadata": {},
   "source": [
    "<a id=\"33-outlier-detection\"></a> \n",
    "### 3.3. Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d69855-2ecf-44db-b1f2-1a764f815c29",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r\n",
    "In the context of 4G network analysis, the presence of outliers in passive measurement datasets can significantly degrade model training and lead to misleading results (Gil et al., 2019). This section discusses the detection, visualization and handling of outliers in a real-world 4G dataset obtained through the Rohde & Schwarz (R&S) TSMA6 network scanner. In this section two statistical techniques which Interquartile Range (IQR) method and the Z-Score method are examined. To rationalize the application of these two methods, it is important to consider the distributional properties of the dataset. The Z-Score method assumes that the underlying data follow an approximately normal distribution and flags observations as outliers if their standardized values exceed the threshold of |z| > 3 (Rousseeuw and Hubert, 2018). However, real-world network data often exhibit skewness, making the Z-Score less effective in such cases. Besides, the IQR method is nonparametric and more robust to skewed distributions. It detects outliers by identifying values in the first quartile (Q1) minus 1.5 times the interquartile range (IQR) or the third quartile (Q3) plus 1.5 times the IQR (Hubert and Van der Veeken, 2008)\n",
    "</div>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac035c5b-220e-4d04-b049-535dc5065145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Define log scale columns (dBm type columns)\n",
    "log_scale_cols = ['Power', 'SINR', 'RSRP', 'RSRQ']\n",
    "\n",
    "# Convert dB and dBm values to linear scale before applying outlier detection\n",
    "df_log_scaled = df.copy()\n",
    "df_log_scaled[log_scale_cols] = 10 ** (df_log_scaled[log_scale_cols] / 10)  # Convert dB to linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d40b11-c601-4eff-a3d6-2ed9335cc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df):\n",
    "    outlier_counts = pd.DataFrame(index=numeric_cols)\n",
    "    \n",
    "    # IQR Method\n",
    "    Q1 = df[numeric_cols].quantile(0.25)\n",
    "    Q3 = df[numeric_cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    iqr_outliers = ((df[numeric_cols] < (Q1 - 1.5 * IQR)) | (df[numeric_cols] > (Q3 + 1.5 * IQR))).sum()\n",
    "    outlier_counts['IQR'] = iqr_outliers\n",
    "    \n",
    "    # Z-Score Method\n",
    "    z_scores = np.abs(zscore(df[numeric_cols]))\n",
    "    z_outliers = (z_scores > 3).sum(axis=0)\n",
    "    outlier_counts['Z-Score'] = z_outliers\n",
    "    \n",
    "    return outlier_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3cefd-38c0-4bf1-9afc-6c3b0bd7f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply outlier detection on log-transformed dataset\n",
    "outlier_results = detect_outliers(df_log_scaled)\n",
    "outlier_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b8dbb-0602-4f4e-b28a-baadd89fd134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(outlier_counts):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Visualize the outlier detection results using a bar plot\n",
    "    outlier_counts.plot(kind='bar', figsize=(15, 6), width=0.8, colormap='viridis')\n",
    "    \n",
    "    # Set plot title and axis labels\n",
    "    plt.title('Outlier Count by Detection Method', fontsize=14)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Number of Outliers', fontsize=12)\n",
    "    \n",
    "    # Rotate column names on the X-axis for better readability\n",
    "    plt.xticks(rotation=75, ha='right', fontsize=10)\n",
    "    \n",
    "    # Add grid lines to improve readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.legend(title='Detection Method')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0b71d-c7d0-4538-b4e0-e5bc9b1287aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(outlier_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4013eaf-b69f-4e2b-bd34-5083950830a4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The figure above illustrates the outlier count by applied two detection methods. Before the outlier detection, Power, SINR, RSRP, RSRQ columns were examined by log transformation since they were in dB and dBm types. This stage was a necessary step for detection on a linear basis. The results show that the IQR method identifies significantly more outliers than the Z-Score method for most attributes. For example, attributes such as Band, RSRP, Power, and SINR exhibit exceptionally high outlier frequencies under the IQR method, exceeding 90,000 for the Power column, for example. In contrast, the Z-Score method is more conservative and detects fewer outliers overall. The discrepancy between the two methods is particularly evident for the Band feature, where the IQR method flags more than 120,000 outliers, while Z-Score does not detect any. This suggests that Band has a non-Gaussian distribution, which limits the effectiveness of Z-Score in capturing outliers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468a54b-1f91-4685-a6bd-45c14f589410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(df, outlier_counts, method=\"IQR\"):\n",
    "    \"\"\"\n",
    "    Plots outliers using IQR or Z-Score method.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing numerical features.\n",
    "    outlier_counts (pd.DataFrame): A DataFrame with outlier counts from IQR/Z-Score.\n",
    "    method (str): \"IQR\" or \"Z-Score\" to determine which method's outliers to plot.\n",
    "    \"\"\"\n",
    "    if method not in [\"IQR\", \"Z-Score\"]:\n",
    "        raise ValueError(\"Invalid method! Choose 'IQR' or 'Z-Score'.\")\n",
    "    \n",
    "    outliers = outlier_counts[method].sort_values(ascending=False)  # Sort outliers\n",
    "    top_features = outliers[outliers > 0].index  # Only features with outliers\n",
    "    \n",
    "    if len(top_features) == 0:\n",
    "        print(f\"No outliers detected using {method} method!\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 10), gridspec_kw={'height_ratios': [2, 1]})\n",
    "    \n",
    "    # 1️ Boxplot + Stripplot (Highlight outliers in red)\n",
    "    plt.sca(axes[0])\n",
    "    sns.boxplot(data=df[top_features], orient='h', showfliers=False, palette=\"coolwarm\")\n",
    "    sns.stripplot(data=df[top_features], orient='h', color='red', alpha=0.5, jitter=True, size=3)\n",
    "    plt.title(f\"Feature Distributions & Outliers ({method} Method)\", fontsize=14)\n",
    "    plt.xlabel(\"Feature Values\")\n",
    "    \n",
    "    # 2️ Bar Plot (Outlier Count per Feature)\n",
    "    plt.sca(axes[1])\n",
    "    sns.barplot(x=outliers.index, y=outliers.values, palette=\"viridis\")\n",
    "    plt.xticks(rotation=75)\n",
    "    plt.ylabel(\"Number of Outliers\")\n",
    "    plt.title(f\"Outlier Count per Feature ({method} Method)\", fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95106909-3e0b-44eb-b5a4-c590fda6e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR Outliers\n",
    "plot_outliers(df_log_scaled, outlier_results, method=\"IQR\")\n",
    "# Z-Score Outliers\n",
    "plot_outliers(df_log_scaled, outlier_results, method=\"Z-Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f39a2-8122-41b5-b015-e1d11722321b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r",
    "As seen in figures above, features such as Speed, Distance, and Altitude contain significant high-level outliers, indicating irregularities in user mobility or measurements. Signal-related features such as RSRP and SINR also show significant variability, especially in the lower tail, which may correspond to weak signal conditions or hardware measurement errors. The fact that Z-Score fails to capture most of these anomalies highlights its limitations for skewed or heavy-tailed variables. Meanwhile, the IQR method successfully identifies these irregular data points, making it a more suitable choice for this context. \n",
    "\r\n",
    "</di>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f2e2a-e42d-4645-8fc9-59202db7929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e84b2-fa72-412c-8ab5-56b7e148d1f6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "Based on the IQR and Z-Score methods, the features Altitude, RSRP, RSRQ, and distance were flagged as suspicious due to the presence of extreme values. As shown in figure below, their histograms and boxplots were analysed to better understand these distributions. This visual inspection guided the outlier handling process. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb0e7b-5116-4a8e-b932-d34db77fd3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_cols = ['Altitude', 'RSRP', 'RSRQ', 'distance']\n",
    "\n",
    "# visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Loop through each column and create a subplot for histograms and boxplots\n",
    "for i, col in enumerate(suspicious_cols):\n",
    "    plt.subplot(len(suspicious_cols), 2, 2 * i + 1)\n",
    "    sns.histplot(df[col], bins=50, kde=True, color='royalblue')\n",
    "    plt.axvline(df[col].mean(), color='red', linestyle='dashed', linewidth=1)  # Mean line\n",
    "    plt.axvline(df[col].median(), color='green', linestyle='dashed', linewidth=1)  # Median line\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    plt.subplot(len(suspicious_cols), 2, 2 * i + 2)\n",
    "    sns.boxplot(x=df[col], color='lightcoral')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82392bf-1f7d-405c-9ba2-e498d7f6c0b2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r",
    "In outlier handling part, two domain-informed transformations were applied to the features RSRQ and Altitude to address specific types of invalid or physically impossible values. \n",
    "First, RSRQ feature was examined. Any value falling below -30 dB was capped at -30 dB. This decision is grounded in telecommunication engineering principles. In practical LTE and 4G deployments, RSRQ values typically range between -3 dB (excellent signal quality) and -20 dB (poor signal quality) (CableFree, 2025). Values below -30 dB are not only extremely rare but also indicative of measurement errors or sensor noise. Thus, replacing such values with a lower bound of -30 dB serves as a form of value clipping, which helps retain the record while mitigating the influence of spurious signal readings on downstream models. To sum up, this approach preserves the sample distribution while ensuring the integrity of signal quality metrics remains within technically plausible boundaries. After that, Altitude feature was examined. All values below 0 were set to 0. Altitude, representing elevation above sea level, should theoretically be a non-negative quantity. While negative altitudes are possible in specific geological contexts, such cases are unlikely in geographical conditions of Rome (Topographic Map, 2025). Therefore, assigning a minimum of zero ensures physical plausibility and helps prevent the propagation of erroneous geographic data into models that might rely on spatial context.\r\n",
    "\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52175232-f8ab-4ded-b476-0e25449c7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Limit RSRQ to -30 dB if it is below -30\n",
    "df['RSRQ'] = df['RSRQ'].apply(lambda x: -30 if x < -30 else x)\n",
    "\n",
    "#  Set Altitude to 0 if it is below 0\n",
    "df['Altitude'] = df['Altitude'].apply(lambda x: 0 if x < 0 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f8c87-62be-4742-b8b4-85e3f204247d",
   "metadata": {},
   "source": [
    "<a id=\"34-encoding--normalization\"></a> \n",
    "### 3.4. Encoding & Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99a2b0-eef4-467d-bddd-b8cc6c4b899b",
   "metadata": {},
   "source": [
    "In this step label encoding has performed for categorical variables and standart scaler has used for numerical variables.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11bd23-a746-4c90-9dcc-0aecf78f553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = df.copy() # dataframe for Explanatory Data Analysis\n",
    "df_cla= df.copy()  # dataframe for Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48809521-e495-4b69-a97f-b967dc5add81",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\", \"int32\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ed8ed-d434-4891-9416-c6d238134163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label encoder object \n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    df_cla[col] = le.fit_transform(df_cla[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee39082-0181-4222-b972-7622f5764d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df_cla[col] = scaler.fit_transform(df_cla[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491e212-e636-43a9-aff6-77f6749ba390",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_columns = len(categorical_cols) + len(numeric_cols)\n",
    "print(f\"Categorical Columns: {len(categorical_cols)}, Numerical Columns: {len(numeric_cols)}, Total Columns: {total_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a890f8-4466-48ed-80b7-9d74bed10194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoded & Scaled DataFrame:\")\n",
    "df_cla.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975dc36-9df1-4175-a78c-8a5b1ac845dd",
   "metadata": {},
   "source": [
    "<a id=\"35-data-visualizations\"></a> \n",
    "### 3.5. Data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f6284-467b-4707-b0ad-63a6e0705ca5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "In this study, Pearson's Correlation was used to analyse the numerical columns. In this case, the correlation matrix systematically quantifies linear relationships between multiple numeric variables by evaluating their relationships on a standard scale and this process ranging from -1 to 1 (Alshammari, 2024). Furthermore, this step facilitates informed decision making in statistical modelling and feature selection by allowing them to detect dependencies and identify patterns in the data. The results of the applied correlation matrix provided a comprehensive overview of the linear relationships between numerical features in the dataset. Figure 2.10 illustrates the correlation matrix of numerical features. For instance, power shows strong positive correlations with RSRP (0.97) and RSRQ (0.77). This indicates that signal strength measurements are closely linked and may affect each other. In addition, SINR also shows a moderate correlation with Power (0.77), indicating its dependence on signal strength quality.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816dbd58-f7b0-4675-baaf-8541c98ab42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(df_eda[numeric_cols].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Numerical Features\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c805b-fa97-4018-a644-f1e631adba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "# Cramer's V Matrix for Categorical Features\n",
    "cramers_results = pd.DataFrame(index=categorical_cols, columns=categorical_cols)\n",
    "\n",
    "for col1 in categorical_cols:\n",
    "    for col2 in categorical_cols:\n",
    "        if col1 != col2:\n",
    "            cramers_results.loc[col1, col2] = cramers_v(df_eda[col1], df_eda[col2])\n",
    "        else:\n",
    "            cramers_results.loc[col1, col2] = 1.0  # perfect correlation with itself\n",
    "\n",
    "# Convert to float for heatmap\n",
    "cramers_results = cramers_results.astype(float)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cramers_results, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "plt.title(\"Cramér’s V Correlation Matrix (Categorical Features)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1623c03-2c52-410a-bc8d-941a6d29411a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r",
    "In this work, Cramér’s V was used to assess the strength of the relationship between categorical variables. Cramér’s V is derived from the chi-square statistic, and this provides a normalized measure that ranges from 0 to 1 (perfect correlation) (Skotarczak et al., 2019). The figure above indicates the heatmap visualizes Cramér’s V values between pairs of categorical variables. Moreover, darker tones indicate stronger relationships, while lighter tones indicate weaker or negligible relationships. For example, the Cramér’s V value between the variables EARFCN and PCI is 0.82, indicating a strong relationship between them. In detail, it can be said that the same or similar physical cell identities (PCIs) are often repeated for a given frequency band (EARFCN).\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc907f-5445-490a-9e4a-3e5a3c5a9498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting individual features for plotting\n",
    "x = df_eda['RSRP']  # Reference Signal Received Power\n",
    "y = df_eda['RSRQ']  # Reference Signal Received Quality\n",
    "z = df_eda['Power']  # Power measurement\n",
    "\n",
    "# Creating a 3D figure to visualize the relationship between RSRP, RSRQ, and Power\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')  # Defining a 3D axis for the plot\n",
    "\n",
    "# Generating a 3D scatter plot with color intensity mapped to the 'Power' feature\n",
    "sc = ax.scatter(x, y, z, c=z, cmap='viridis', s=50, alpha=0.7)\n",
    "\n",
    "# Setting axis labels to specify the parameters being plotted\n",
    "ax.set_xlabel('RSRP')  # X-axis represents received power strength\n",
    "ax.set_ylabel('RSRQ')  # Y-axis represents received signal quality\n",
    "ax.set_zlabel('Power')  # Z-axis represents power measurement\n",
    "ax.set_title('3D Scatter Plot of RSRP, RSRQ, and Power')  # Title for clarity\n",
    "\n",
    "# Adding a color bar to indicate intensity variations in the Power feature\n",
    "cb = plt.colorbar(sc, ax=ax, shrink=0.5, aspect=10)  # Adjust size and aspect ratio\n",
    "cb.set_label('Power Intensity')  # Label describing the significance of colors\n",
    "\n",
    "# Display the visualization\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17ab14df-e696-4e30-8f25-56dc257010b3",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "A 3D graph of the RSRP, RSRQ and Power columns was plotted as seen in figure given above. Each data point is color-coded according to its power density, with yellow indicating higher power levels and purple indicating lower levels. In detail, a clear positive trend is observed between RSRP and Power. This indicates that stronger signal strength is associated with better signal reception. Moreover, RSRQ shows a consistent pattern, indicating that signal quality improves with increasing power. The density and alignment of the points emphasize the non-random structure. Furthermore, this supports the assumption of a significant correlation between these features. This visualization is particularly useful for identifying patterns in signal performance and can assist in feature selection for modelling or clustering tasks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817bf327-4171-4320-824b-4a51ce1f34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_eda, x=\"distance\", y=\"Power\", alpha=0.4, color=\"mediumseagreen\")\n",
    "\n",
    "plt.title(\"Relationship between Distance and Power\", fontsize=14)\n",
    "plt.xlabel(\"Distance (meters)\")\n",
    "plt.ylabel(\"Power (dBm)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67e0bc0d-2f30-475d-afe9-30b18cef0591",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The scatter plot in figure given above shows the inverse relationship between signal strength and distance. As the distance from the source increases, the received power decreases continuously, highlighting the signal attenuation in space. This negative correlation reflects the common wireless communication behaviour where greater distances result in higher path loss, which in turn reduces signal strength.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39784644-f91a-4924-87e5-3e5403f9e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# User locations (marked with a star '*')\n",
    "plt.scatter(df_eda['Longitude'], df_eda['Latitude'], \n",
    "            s=20, c='blue', marker='*', label='User Location', alpha=0.5)\n",
    "\n",
    "# Cell tower locations (marked with a triangle '^')\n",
    "plt.scatter(df_eda['cellLongitude'], df_eda['cellLatitude'], \n",
    "            s=20, c='red', marker='^', label='Cell Tower Location', alpha=0.7)\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "# Title for better readability\n",
    "plt.title('User vs. Cell Tower Locations')\n",
    "\n",
    "# Legend for clarity\n",
    "plt.legend()\n",
    "\n",
    "# Grid for better visualization\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust layout for optimal spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d26b92-c85a-46d3-bc51-885e8bafd8b3",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The correlation matrix revealed that there is a high correlation between Latitude, Longitude, cellLongitude and cellLatitude. The figure given above illustrates the locations of user locations and cell locations. This figure shows the location information as coordinates and reveals their locations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10817c7e-d890-439e-ba33-5a66a970c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping data by frequency and calculating the mean of Power and RSRP\n",
    "grouped = df_eda.groupby('Frequency')[['Power', 'RSRP']].mean().reset_index()\n",
    "\n",
    "# Set the plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the line charts\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Line chart for Power\n",
    "sns.lineplot(data=grouped, x='Frequency', y='Power', label='Power (dBm)', color='darkorange')\n",
    "\n",
    "# Line chart for RSRP\n",
    "sns.lineplot(data=grouped, x='Frequency', y='RSRP', label='RSRP (dBm)', color='steelblue')\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Average Power and RSRP vs Frequency')\n",
    "plt.xlabel('Frequency (MHz)')\n",
    "plt.ylabel('Signal Strength (dBm)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064757e6-4d98-49ac-b6b9-f99feb23532c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The figure given above illustrates the relationship between average Power and RSRP across different frequency bands. As frequency increases, both Power and RSRP generally decrease. This indicates signal attenuation at higher frequencies. A sharp improvement is observed around 1800 MHz, likely due to optimized network configurations or favourable propagation characteristics within that band.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b04cd-6b64-4a72-bcaa-56891a508edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot to compare scenario distribution between weekdays and weekends\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(data=df_eda, x='scenario', hue='is_weekend')\n",
    "plt.title('Scenario Count by Weekend Indicator')\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Is Weekend')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b000c-e482-4991-8688-22d1838e59d5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The chart illustrates scenario counts split by weekday and weekend. All scenarios IS, OD, and OW occur more frequently on weekdays. OW shows the highest weekday activity, while OD has the most balanced distribution. This suggests reduced network activity during weekends across all scenarios.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bdf203-7b06-46a8-927e-54f5e9108c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_eda # delete df_eda dataframe after Explanatory Data Analysis finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0429c-565c-4574-b97c-2d5b13535e45",
   "metadata": {},
   "source": [
    "<a id=\"4-clustering-analysis-task-2\"></a> \n",
    "# 4. Clustering Analysis (Task 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7105d71-1b3d-4432-9796-3e30a3460f9c",
   "metadata": {},
   "source": [
    "<a id=\"41-k-means-clustering\"></a> \n",
    "### 4.1. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f42e6b-e8b7-4aa0-b1d8-244851f07d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.05, random_state=42) # use 5% of data for only clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d6374-4f11-4c11-a6a8-6e9c2d044f84",
   "metadata": {},
   "source": [
    "Considering the experimental setup, a random 5% portion of the dataset was taken only for clustering task in terms of time and processing power efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f047f0-bed5-42a9-bc0a-1c159097fa56",
   "metadata": {},
   "source": [
    "#### K-Means with Using Encoded Catogorical Columns and Scaled Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040dd653-b501-4652-9314-1818009fd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Dataframe ---\n",
    "X = df[numeric_cols + categorical_cols ]  # In this part both numerical and categorical columns used for clustering\n",
    "\n",
    "# --- Elbow Method to find optimal k ---\n",
    "inertia = []\n",
    "K = range(2, 11)\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        init='k-means++',   # KMeans++ initialization\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (Within-Cluster Sum of Squares)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9850d-b0d3-437a-ae9c-ccc28a235b6d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "In this study, normalized numeric data was used together with categorical variables encoded in the first K means algorithm. However, since the structure of categorical variables does not fit k-means, a noticeable performance loss was noticed. As seen in above, the cluster numbers and silhouette scores of K-means performed with all different data are seen. For this reason, the study was performed only with numerical data.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722bd83-e3d6-4d3e-83f3-67017e093ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Silhouette Scores for different k values ---\n",
    "silhouette_scores = []\n",
    "K = range(2, 11)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K, silhouette_scores, 'go-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for different k')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c7e48-0091-4545-a65e-70540cf61fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K:\n",
    "    print(f\"Cluster Number: {k}, Silhouette Score: {silhouette_scores[k-2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdca3a8-1aad-4dae-b248-212583ce870f",
   "metadata": {},
   "source": [
    "#### K-Means with Using Scaled Numerical Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d18986-acd4-452c-9b72-d2d4b1bb77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessed DataFrame ---\n",
    "X = df[numeric_cols]  # only numerical variables\n",
    "\n",
    "# --- Elbow Method to find optimal k ---\n",
    "inertia = []\n",
    "K = range(2, 11)\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        init='k-means++',   # KMeans++ initialization\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (Within-Cluster Sum of Squares)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca1ab0-fcd2-4efe-b544-0e52bdbe4529",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r",
    "Based on the elbow method illustrated in plot above, the optimal number of clusters appears to be 3. At this point, there is a significant reduction in the within-cluster sum of squares (inertia), and the rate of decrease slows down beyond this value. In addition, as Shi et al. (2021) pointed this inflection point indicates a balance between model complexity and performance. Therefore, selecting three clusters is likely to provide meaningful separation without overfitting the data.\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615ac99-2b7f-4454-a14f-5f50665b7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Silhouette Scores for different k values ---\n",
    "silhouette_scores = []\n",
    "K = range(2, 11)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K, silhouette_scores, 'go-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for different k')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef743a9-521d-4ed0-a8e0-bb206ef5b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K:\n",
    "    print(f\"Cluster Number: {k}, Silhouette Score: {silhouette_scores[k-2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe771243-7b63-4a56-b0f5-31a535ae18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fiting final KMeans model  ---\n",
    "optimal_k =  3\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, init='k-means++', n_init=10, random_state=42)\n",
    "cluster_labels = kmeans_final.fit_predict(X)\n",
    "\n",
    "# Add cluster labels to a new column\n",
    "X['kmeans_cluster'] = cluster_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7968d85-b12c-479f-8a3e-4356bcc4149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PCA for visualization ---\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X)\n",
    "\n",
    "df_pca = pd.DataFrame(pca_result, columns=[\"PC1\", \"PC2\"])\n",
    "df_pca[\"Cluster\"] = cluster_labels\n",
    "\n",
    "# --- Scatter plot with clusters ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_pca, x=\"PC1\", y=\"PC2\", hue=\"Cluster\", palette=\"Set2\", s=50)\n",
    "plt.title(\"K-Means Clustering Visualized with PCA\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1266e-79cf-4e5f-88c1-08ff5595987d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The figure given above visualizes clustering using PCA, where data points are projected onto two principal components. The three clusters (0, 1, and 2) show clear separation, indicating that the algorithm successfully identified distinct groups in the dataset with minimal overlap.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9fbbbb-83ca-4b1c-b0c8-1ddb95490399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['KMeans_Cluster'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42c395-7e2f-47df-8999-1a50719b2b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_cols + [\"KMeans_Cluster\"]].groupby('KMeans_Cluster').mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1958219-89c9-49e8-866f-f532a4481ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  \n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # Create  2x2 grid\n",
    "\n",
    "# List of signal metrics to visualize\n",
    "metrics = ['RSRP', 'RSRQ', 'SINR', 'Power']\n",
    "\n",
    "# Loop through each metric and plot on respective subplot\n",
    "for ax, col in zip(axes.flat, metrics):  \n",
    "    sns.boxplot(data=df, x='KMeans_Cluster', y=col, palette=cluster_colors, ax=ax)  # Apply custom colors\n",
    "    ax.set_title(f\"{col} Distribution - Based on KMeans Clustering\")  # Set individual titles\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing for better visualization\n",
    "plt.show()  # Display "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c5304-7286-424e-8458-38e53807b7b7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The figure above shows the distribution of RSRP, RSRQ, SINR and Power across three clusters. Each subplot shows a clear variation in signal characteristics across clusters. It also highlights the effectiveness of the clustering approach in distinguishing patterns in the dataset. Cluster 2 consistently shows the poorest signal quality with the lowest median values for all four metrics, especially RSRP and Power, indicating poorer coverage. Besides, Cluster 0 appears to represent regions with better signal conditions reflected by higher median values. The clear separation in SINR and RSRQ values further supports the ability of the clustering model to segment data according to quality of service.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f788b6-a92b-4959-9aa9-2d5b0f3592ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of measurement points based on geographic location\n",
    "sns.scatterplot(data=df, x='Longitude', y='Latitude', hue='KMeans_Cluster', palette='tab10')\n",
    "\n",
    "plt.title(\"Cluster Distribution of Measurement Points by Location\")  # Set plot title\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffff6a3-f73d-43da-883a-0c15be18f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  # Blue, Orange, Green\n",
    "\n",
    "# Box plot showing base station distance distribution per cluster\n",
    "sns.boxplot(data=df, x='KMeans_Cluster', y='distance', palette=cluster_colors)\n",
    "\n",
    "plt.title(\"Cell Station Distance - Cluster-Based Distribution\")  # Set plot title\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07636deb-19c2-4f2c-a78a-ee9ee85fe3a1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r",
    "The plot above shows the distribution of cell location distances in the three clusters defined by K-Means clustering. Cluster 2 exhibits significantly higher distance values, including a wide range and a considerable number of outliers. It also shows that users in this cluster are generally farther from the cell location. In contrast, Clusters 0 and 1 have more compact distributions with lower median distances, indicating closer location to the cell station.\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f967d1a-b504-44d1-a18c-5330050dce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot showing speed distribution across KMeans clusters\n",
    "sns.violinplot(data=df, x='KMeans_Cluster', y='Speed')\n",
    "\n",
    "plt.title(\"Speed - Cluster-Based Distribution\")  # Set plot title\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f0e30-25c2-4390-a80f-5a8ff96a379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot showing measurement density by hour and clustered with KMeans\n",
    "sns.countplot(data=df, x='hour', hue='KMeans_Cluster', palette='Set2')\n",
    "\n",
    "plt.title(\"Measurement Density by Hour - Cluster-Based\")  # Set plot title\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588ec23-98dc-4995-9e7c-8584e48768cf",
   "metadata": {},
   "source": [
    "<a id=\"42-agglomerative-clustering\"></a> \n",
    "### 4.2. Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27178c00-058d-40f9-873c-eeaccd98079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate a Dendrogram using Ward linkage ---\n",
    "linked = linkage(X, method='ward')  # Ward minimizes variance within clusters\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linked, truncate_mode='level', p=5)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Truncated)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa159d2-33f0-4bd1-9ebd-dc645e6ee0f5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r",
    "The figure above illustrates the hierarchical clustering dendrogram. This plot visually represents the nested grouping of data points according to their pairwise distances. In the figure, vertical lines indicate the distance at which clusters merge, and longer vertical lines reflect greater dissimilarity. Observe the largest vertical distances that are not crossed by horizontal lines, and a natural division into three main clusters is determined, represented by distinct colours. This visual gap, also known as the “distance threshold,” supports the selection of the optimal number of clusters without requiring prior assumptions (Li et al., 2022). Therefore, the dendrogram plays an important role for defining the optimal number of clusters in agglomerative clustering. So, the optimal number of clusters was determined as three. On the other hand, this is justified by the clear separation observed at a high linkage distance, where the data split into three main branches before large vertical mergers occur. This significant jump in linkage distance indicates a natural cutoff point.\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e927d9-e219-4e9a-9e00-31fb20c98583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate cluster quality using Silhouette Score ---\n",
    "silhouette_scores = []\n",
    "K = range(2, 8)\n",
    "\n",
    "for k in K:\n",
    "    model = AgglomerativeClustering(n_clusters=k)\n",
    "    labels = model.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(K, silhouette_scores, 'ro-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Agglomerative Clustering')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afaff59-43c3-41f1-9a84-da99f6c2ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit Agglomerative Model with Optimal k ---\n",
    "optimal_k = 3  # based on silhouette and dendrogram\n",
    "agg_model = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "agg_labels = agg_model.fit_predict(X)\n",
    "\n",
    "# Add cluster labels to DataFrame\n",
    "df['Agglo_Cluster'] = agg_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa3f70-e6d8-4a73-87fe-91310f0765e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PCA + Cluster Visualization ---\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X)\n",
    "\n",
    "df_pca_agg = pd.DataFrame(pca_result, columns=[\"PC1\", \"PC2\"])\n",
    "df_pca_agg[\"Cluster\"] = agg_labels\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_pca_agg, x=\"PC1\", y=\"PC2\", hue=\"Cluster\", palette=\"Set2\", s=50)\n",
    "plt.title(\"Agglomerative Clustering Visualized with PCA\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f468cd-0285-4b35-acb9-eecf80915f3c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "Figure given above visualizes clustering using PCA, where data points are projected onto two principal components. The three clusters (0, 1, and 2) show clear separation, indicating that the algorithm successfully identified distinct groups in the dataset with minimal overlap.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608d65e-a3e2-48ae-b360-02b4278a2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_cols + [\"Agglo_Cluster\"]].groupby('Agglo_Cluster').mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921a039-c935-4882-bbba-af0776032ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  \n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # Create a 2x2 grid\n",
    "\n",
    "# List of signal metrics to visualize\n",
    "metrics = ['RSRP', 'RSRQ', 'SINR', 'Power']\n",
    "\n",
    "# Loop through each metric and plot on respective subplot\n",
    "for ax, col in zip(axes.flat, metrics):  \n",
    "    sns.boxplot(data=df, x='Agglo_Cluster', y=col, palette=cluster_colors, ax=ax)  # Apply custom colors\n",
    "    ax.set_title(f\"{col} Distribution - Based on Agglomerative Clustering\")  # Set individual titles\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing for better visualization\n",
    "plt.show()  # Display the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdec5a7-a296-484a-87fc-72c1b8145edc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r",
    "Plots given above indicate the distribution of RSRP, RSRQ, SINR, and Power across three clusters derived from agglomerative clustering. Each subplot reveals distinct variations in signal characteristics among the clusters. Cluster 1 exhibits the weakest signal conditions, marked by the lowest median values in RSRP and Power. Moreover, Cluster 2 demonstrates stronger signal characteristics with higher median values. The separation in RSRQ and SINR across the clusters further confirms the clustering model’s capacity to differentiate data based on performance.\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792fdf5f-11af-48f4-8981-e7cc3ac5840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean values for Distance and Frequency per cluster\n",
    "cluster_means = df.groupby(\"Agglo_Cluster\")[[\"distance\", \"Frequency\"]].mean().reset_index()\n",
    "\n",
    "# Define custom color palette\n",
    "cluster_colors = [\"#1f77b4\", \"#ff7f0e\"]  # Blue for Distance, Orange for Frequency\n",
    "\n",
    "# Create bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set width for bars\n",
    "bar_width = 0.4  \n",
    "x = np.arange(len(cluster_means[\"Agglo_Cluster\"]))  # X positions\n",
    "\n",
    "# Plot Distance bars\n",
    "ax.bar(x - bar_width/2, cluster_means[\"distance\"], width=bar_width, label=\"Distance\", color=cluster_colors[0])\n",
    "\n",
    "# Plot Frequency bars\n",
    "ax.bar(x + bar_width/2, cluster_means[\"Frequency\"], width=bar_width, label=\"Frequency\", color=cluster_colors[1])\n",
    "\n",
    "# Add labels on top of bars\n",
    "for i in range(len(cluster_means)):\n",
    "    ax.text(x[i] - bar_width/2, cluster_means[\"distance\"][i], f\"{cluster_means['distance'][i]:.2f}\", ha='center', va='bottom', fontsize=11, fontweight=\"bold\")\n",
    "    ax.text(x[i] + bar_width/2, cluster_means[\"Frequency\"][i], f\"{cluster_means['Frequency'][i]:.2f}\", ha='center', va='bottom', fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(cluster_means[\"Agglo_Cluster\"])  # Cluster labels\n",
    "ax.set_xlabel(\"Agglo Cluster\")\n",
    "ax.set_ylabel(\"Average Value\")\n",
    "ax.set_title(\"Distance & Frequency Distribution per Cluster\")\n",
    "ax.legend(title=\"Metrics\")  # Add legend\n",
    "\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a658cd0-7a0f-4a15-9338-78764c21983d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "Plot given above illustrates the average values of Distance and Frequency in the three clusters. Cluster 1 stands out with the highest average distance (2769.14 meters), indicating that it represents data points located further away from the measurement source or cell towers. In contrast, Cluster 0 is characterized by the highest average frequency (2201.23 MHz) but the lowest distance (341.19 meters). Cluster 2 shows relatively balanced values with moderate averages in both metrics. These differences support the idea that clustering effectively captures the different spatial and spectral utilization patterns in the dataset.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687009a-3dee-4653-9950-2f09883211b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of measurement points based on geographic location\n",
    "sns.scatterplot(data=df, x='Longitude', y='Latitude', hue='Agglo_Cluster', palette='tab10')\n",
    "\n",
    "plt.title(\"Cluster Distribution of Measurement Points by Location\")  # Set plot title\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d9530-7761-4c48-be34-356a7b0c735e",
   "metadata": {},
   "source": [
    "<a id=\"43-clustering-comparison\"></a> \n",
    "### 4.3 Clustering Models' Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d14a7f-3604-4acd-ad3f-c7568959925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_cols + [\"Agglo_Cluster\", \"KMeans_Cluster\"]].groupby(['Agglo_Cluster', 'KMeans_Cluster']).mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390765fe-5ca5-4d5c-aef5-a2ae6894a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences for each cluster in K-Means and Agglomerative clustering\n",
    "kmeans_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "agglo_counts = pd.Series(agg_labels).value_counts().sort_index()\n",
    "\n",
    "# Define color palettes for clusters to ensure visual differentiation\n",
    "kmeans_colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  # Blue, orange, and green for K-Means\n",
    "agglo_colors = [\"#2ca02c\", \"#ff7f0e\", \"#1f77b4\"]  # Green, orange, and blue for Agglomerative\n",
    "\n",
    "# Create a figure with two subplots to visually compare cluster distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # Two plots in one row for easy side-by-side comparison\n",
    "\n",
    "# Plot the pie chart for K-Means clustering distribution\n",
    "axes[0].pie(kmeans_counts, labels=kmeans_counts.index, autopct=lambda p: f\"{p:.1f}%\", \n",
    "            colors=kmeans_colors, startangle=140, textprops={'fontsize': 14, 'fontweight': 'bold'})\n",
    "axes[0].set_title(\"K-Means Cluster Distribution\", fontsize=16, fontweight=\"bold\")  # Set the title for better readability\n",
    "\n",
    "# Plot the pie chart for Agglomerative clustering distribution\n",
    "axes[1].pie(agglo_counts, labels=agglo_counts.index, autopct=lambda p: f\"{p:.1f}%\", \n",
    "            colors=agglo_colors, startangle=140, textprops={'fontsize': 14, 'fontweight': 'bold'})\n",
    "axes[1].set_title(\"Agglomerative Cluster Distribution\", fontsize=16, fontweight=\"bold\")  # Consistent title formatting\n",
    "\n",
    "# Adjust layout to ensure plots don't overlap and have proper spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9db6e-bddf-4853-8148-f249a7063cf9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The pie charts given above illustrates a comparison of cluster sizes generated by the K-Means and Agglomerative algorithms. Although both methods include 3 clusters, the distribution of data points across clusters differs significantly. For instance, K-Means generated a dominant cluster at label 1, while Agglomerative clustering concentrated more points at label 0. Moreover, the Silhouette Scores for both clustering methods are 0.62. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced84a67-09d7-469d-931b-5be9e01c061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features to compare across clustering methods\n",
    "features = ['RSRP', 'RSRQ', 'Power', 'distance', 'SINR', 'Frequency', 'Speed']\n",
    "\n",
    "# Convert cluster labels to string type for plotting\n",
    "df['Agglo_Cluster'] = df['Agglo_Cluster'].astype(str)\n",
    "df['KMeans_Cluster'] = df['KMeans_Cluster'].astype(str)\n",
    "\n",
    "# Loop through each feature for plotting\n",
    "for feature in features:\n",
    "    # Prepare Agglomerative clustering data\n",
    "    df_agglo = df[[feature, 'Agglo_Cluster']].copy()\n",
    "    df_agglo['Cluster_Label'] = 'Agglo_' + df_agglo['Agglo_Cluster']\n",
    "    df_agglo['Method'] = 'Agglomerative'\n",
    "\n",
    "    # Prepare KMeans clustering data\n",
    "    df_kmeans = df[[feature, 'KMeans_Cluster']].copy()\n",
    "    df_kmeans['Cluster_Label'] = 'KMeans_' + df_kmeans['KMeans_Cluster']\n",
    "    df_kmeans['Method'] = 'KMeans'\n",
    "\n",
    "    # Rename feature column to unify\n",
    "    df_agglo.rename(columns={feature: 'Value'}, inplace=True)\n",
    "    df_kmeans.rename(columns={feature: 'Value'}, inplace=True)\n",
    "\n",
    "    # Combine both datasets\n",
    "    combined_df = pd.concat([df_agglo, df_kmeans], axis=0)\n",
    "\n",
    "    # Plot using seaborn boxplot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=combined_df, x='Cluster_Label', y='Value', hue='Method', palette='Set2')\n",
    "\n",
    "    # Title and labels\n",
    "    plt.title(f'Comparison of {feature} Across Clustering Methods', fontsize=14)\n",
    "    plt.xlabel('Cluster Labels by Method')\n",
    "    plt.ylabel(feature)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Method', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c744d06-fb87-4cfa-8f23-35f8c45d5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-tab of Band frequency per cluster type\n",
    "band_counts = pd.crosstab(df['Band'], df['Agglo_Cluster'])\n",
    "band_counts_kmeans = pd.crosstab(df['Band'], df['KMeans_Cluster'])\n",
    "\n",
    "# Create side-by-side bar charts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot Agglomerative clustering\n",
    "band_counts.plot(kind='bar', stacked=True, colormap='Paired', ax=axes[0])\n",
    "axes[0].set_title(\"Distribution of Band across Agglomerative Clusters\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Band\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"Count\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].legend(title=\"Agglo_Cluster\")\n",
    "\n",
    "# Plot KMeans clustering\n",
    "band_counts_kmeans.plot(kind='bar', stacked=True, colormap='Paired', ax=axes[1])\n",
    "axes[1].set_title(\"Distribution of Band across KMeans Clusters\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Band\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"Count\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].legend(title=\"KMeans_Cluster\")\n",
    "\n",
    "plt.tight_layout()  # Optimize spacing\n",
    "plt.show()  # Display plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c099c8-fdef-45fe-aaea-3e51c24a9c27",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The bar charts given above indicate how different frequency bands are distributed among the clusters identified by the Agglomerative and K-Means algorithms. The clustering distributions are different on a per-cluster basis. For example, Agglomerative clustering shows a stronger dominance of Band 20 in Cluster 2, while K-Means clustering assigns most of the Band 20 data to Cluster 0.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6366c5f-90f0-45e7-848f-2015c74cabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping data by KMeans clusters and calculating the mean of distance and frequency\n",
    "kmeans_grouped = df.groupby('KMeans_Cluster')[['distance', 'Frequency']].mean().reset_index()\n",
    "\n",
    "# Reshaping the data for better visualization (melting columns into a long-format structure)\n",
    "kmeans_grouped = pd.melt(kmeans_grouped, id_vars='KMeans_Cluster', \n",
    "                         var_name='Metric', value_name='AvgValue')\n",
    "\n",
    "# Similarly, grouping data by Agglomerative clusters and computing average distance and frequency\n",
    "agglo_grouped = df.groupby('Agglo_Cluster')[['distance', 'Frequency']].mean().reset_index()\n",
    "\n",
    "# Melting the dataframe to prepare it for visualization\n",
    "agglo_grouped = pd.melt(agglo_grouped, id_vars='Agglo_Cluster', \n",
    "                        var_name='Metric', value_name='AvgValue')\n",
    "\n",
    "# Define color palettes for visual differentiation\n",
    "kmeans_palette = [\"#1f77b4\", \"#ff7f0e\"]  # Blue and orange for KMeans cluster metrics\n",
    "agglo_palette = [\"#2ca02c\", \"#d62728\"]  # Green and red for Agglomerative cluster metrics\n",
    "\n",
    "# Creating a figure with two subplots to compare clustering results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # Arrange plots side by side\n",
    "\n",
    "# Plot for KMeans clustering results\n",
    "sns.barplot(ax=axes[0], data=kmeans_grouped, x='KMeans_Cluster', y='AvgValue', hue='Metric', palette=kmeans_palette)\n",
    "axes[0].set_title('KMeans - Avg Distance & Frequency')  # Setting title for clarity\n",
    "axes[0].set_xlabel('KMeans Cluster')  # Labeling the x-axis\n",
    "axes[0].set_ylabel('Average Value')  # Labeling the y-axis\n",
    "\n",
    "# Plot for Agglomerative clustering results\n",
    "sns.barplot(ax=axes[1], data=agglo_grouped, x='Agglo_Cluster', y='AvgValue', hue='Metric', palette=agglo_palette)\n",
    "axes[1].set_title('Agglomerative - Avg Distance & Frequency')  # Providing an informative title\n",
    "axes[1].set_xlabel('Agglo Cluster')  # Labeling the x-axis\n",
    "axes[1].set_ylabel('Average Value')  # Consistent labeling with the KMeans plot\n",
    "\n",
    "# Adjust layout for better spacing and readability\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d7ffe-efcd-4e8e-9f36-5df7150f7c11",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The figure above illustrates differences in average distance and frequency for each cluster generated by K-Means and Agglomerative clustering. For example, Cluster 2 exhibits the highest average distance about 3000 meter in the K-Means model. This occurs in Cluster 1 of the Agglomerative model. These patterns highlight how each algorithm partitions the data differently in terms of distribution and density. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b8308-4f0a-4e4a-a2ea-099995ee7506",
   "metadata": {},
   "source": [
    "<a id=\"5-classification-task-3\"></a> \n",
    "# 5. Classification (Task 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa9f086-893d-4fc9-9831-7b9684b0472a",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: justify;\">\r",
    "For the binary classification task, the feature MNC (Mobile Network Code) was selected as the target variable. This variable includes the network operator identifier and is quite suitable for binary classification due to its two distinct categories. Moreover, the class distribution is relatively balanced. “Op2\" accounts for about 53.9% of the samples, while “Op1\" represents 46.0%, which helps overcome the class imbalance issues during training. For the multi-class classification task, the scenario variable was selected as the target. This feature divides the measurement context into three categories: Outdoor Walking (OW), Indoor Stationary (IS), and Outdoor Driving (OD). These are important for modelling the changes in network performance in different real-world mobility scenarios. Moreover, the distribution of this variable is sufficiently diverse; OW is 43%, IS is 41% and OD is 16%, which makes it suitable for robust multi-class classification.\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e774b4-5028-4279-843a-6c6214aa23fc",
   "metadata": {},
   "source": [
    "<a id=\"51-binary-classification\"></a> \n",
    "### 5.1. Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d325ccf-8a17-46d8-b62f-a263a9ba753d",
   "metadata": {},
   "source": [
    "#### RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd5a86-cc57-48a4-b36d-13dad123412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_cla.copy()  # using 100% of the dataset\n",
    "del df_cla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432afa9e-177a-4185-be71-55606c0c2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[numeric_cols + categorical_cols].drop(columns=['MNC','CellIdentity','eNodeB.ID','EARFCN']) # dropping target columns and the columns which showed high correlation with target column\n",
    "y = df[\"MNC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8376e-e730-4e40-b446-ee9f3c9d57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # train and test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c070a-e83c-405e-9db0-39078d3eadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train the model\n",
    "rf_model = RandomForestClassifier(\n",
    "    max_depth=2,             \n",
    "    random_state=42,\n",
    "    n_jobs=-1                 \n",
    ")\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d6feb-f8cd-416f-a7e3-b149da058ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1896a-3db8-4393-b0c4-be1f14f8e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy and F1 score\n",
    "acc = accuracy_score(y_test, y_pred_rf)\n",
    "f1 = f1_score(y_test, y_pred_rf, average='binary')  \n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d4a5d8-b9e8-4740-919d-bb7974bf6e87",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r",
    "In this step the max_depth parameter was limited to reduce overfitting risk and the remaining hyperparameters were kept at default values. This approach provided a balance between model complexity and generalizability. After training the model, the results were visualized.\r\n",
    "As shown i nfigurebelow1, the confusion matrix highlights the strong performance of the model with 0.89 accuracy and 0.90 F1 score. These metrics indicate that the model is well balanced not only in accuracy metric but also in terms of precision and recall. The relatively low number of false positives and false negatives further supports its reliability in correctly classifying both classe\n",
    "\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779736e-0712-4279-934c-59e18362ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed13b73-85f7-4b67-bd11-8a9d233fc896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a0aea-195c-4a66-9b08-d6faa92f21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train.columns\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importances[indices][:15], y=features[indices][:15], palette='viridis')\n",
    "plt.title(\"Top 15 Feature Importances - Random Forest\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa3276-6ce3-415e-94f6-b565b57fb76c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r",
    "Figure given above illustrates the top 15 feature importances derived from the Random Forest classification model used in the binary classification task. The most significant feature is n_CellIdentities, followed closely by Frequency and Band. This indicates their critical role in model decision-making. Features such as distance, cellLatitude, and cellLongitude also contribute significantly to the classification outcome. In contrast, variables like day, SINR, and Longitude appear to have minimal impact. These ranking highlights which input variables are most informative for model.\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98410aef-b0ed-460e-80c4-058b3ba7bea9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "In this study, SHAP (SHapley Additive exPlanations) analysis was employed as an additional method for interpreting the Random Forest binary classification model. SHAP assigns each feature an importance value for a particular prediction, offering both global and local interpretability. By quantifying the contribution of each feature to the model's output, SHAP provides a nuanced understanding of feature impacts beyond traditional importance metrics (Lundberg and Lee, 2017). \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c0e1b-c786-4814-a94f-31eba798072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis - Explain model predictions using SHAP values\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Check the shape of SHAP values\n",
    "print(\"SHAP Values Shape:\", np.array(shap_values).shape)\n",
    "\n",
    "if isinstance(shap_values, list):  # If SHAP values are returned as a list\n",
    "    shap_values_selected = shap_values[1]  # Take SHAP values for class \"1\"\n",
    "else:  \n",
    "    shap_values_selected = shap_values[:, :, 1]  # Extract SHAP values for class 1 from a multi-dimensional array\n",
    "\n",
    "# Verify the corrected shape\n",
    "print(\"Corrected SHAP Shape:\", shap_values_selected.shape)\n",
    "\n",
    "# If the shape does not match the expected format, reshape the values\n",
    "if shap_values_selected.shape[1] != X_test.shape[1]:  \n",
    "    shap_values_selected = shap_values_selected.reshape(X_test.shape)\n",
    "\n",
    "# Generate summary plot (Bar format) - Shows feature importance\n",
    "shap.summary_plot(shap_values_selected, X_test, plot_type=\"bar\")\n",
    "\n",
    "# Generate full summary plot - Shows feature impact on predictions\n",
    "shap.summary_plot(shap_values_selected, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a5aca-01d4-4437-984a-8e13108df8c2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The figure given above presents the SHAP value summary plot for the Random Forest binary classification model, providing insight into each feature's impact on model predictions. The plot highlights that n_CellIdentities, Frequency, and Band are the most influential features, with higher SHAP values indicating a stronger effect on the prediction output. The colour gradient represents the feature value, where red denotes high values and blue denotes low values. Notably, high values of Frequency tend to push the prediction towards one class, while low values of distance and cellLatitude are associated with a shift toward the other class.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67a5d4-714e-41d4-8a17-6eb02d94ccbf",
   "metadata": {},
   "source": [
    "### Logistic Regression Classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff5869-b8c4-4d26-b9d0-cca4383a587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "# - Set random_state for reproducibility\n",
    "# - Increase max_iter to ensure convergence\n",
    "log_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf6e07-6293-4c5a-8299-cdc7612b769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_lr = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51a340-e18e-4c0a-a170-b0cce5ba2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))  # Prints overall accuracy\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))  # Precision, recall, F1-score per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2b07d-5aa6-480c-8a24-bf4a9aa2c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix visualization to analyze misclassifications\n",
    "cm = confusion_matrix(y_test, y_pred_lr)  # Generate confusion matrix\n",
    "plt.figure(figsize=(6, 4))  # Set figure size\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')  # Create heatmap \n",
    "plt.xlabel(\"Predicted\")  # Label x-axis\n",
    "plt.ylabel(\"Actual\")  # Label y-axis\n",
    "plt.title(\"Confusion Matrix\")  # Set plot title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932af06-3fe9-45cc-be12-5a5d38b2be64",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The confusion matrix shows that the model correctly predicted 40,987 class 0 instances and 48,332 class 1 instances. Meanwhile, 7,573 false positives and 8,616 false negatives were recorded. These results indicate a well-performing classifier with a good balance between precision and recall. Moreover, Logistic Regression was a successful model with an accuracy result of 0.85.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b65daf-e032-4703-8009-313c20e48943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (Coefficient-Based)\n",
    "# - Logistic Regression coefficients represent the importance of features\n",
    "# - Higher absolute values indicate stronger influence on predictions\n",
    "coefficients = log_model.coef_[0]  # Extract coefficients (for binary classification)\n",
    "feature_names = X_train.columns  # Get feature names\n",
    "\n",
    "# Create a DataFrame to store feature importance\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,  # Feature names\n",
    "    'Coefficient': coefficients,  # Raw coefficients\n",
    "    'Importance': np.abs(coefficients)  # Absolute magnitude for importance ranking\n",
    "}).sort_values(by='Importance', ascending=False)  # Sort by importance\n",
    "\n",
    "# Visualizing feature importance using a bar plot\n",
    "plt.figure(figsize=(10, 6))  # Set figure size\n",
    "sns.barplot(x='Importance', y='Feature', data=coef_df, palette='coolwarm')  \n",
    "plt.title(\"Feature Importance (Logistic Regression Coefficients)\", fontsize=14, fontweight='bold') \n",
    "plt.xlabel(\"Coefficient Magnitude (abs)\", fontsize=12)  \n",
    "plt.ylabel(\"Feature\", fontsize=12)  \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac847d-1196-4519-bc51-d1cbd6335c6c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The plot given above presents the feature importance values based on the absolute magnitude of the coefficients obtained from the Logistic Regression model applied to the binary classification task. Among all the features, Power and RSRP showed the highest importance. This indicated that signal strength features are the main determinants in the classification decision process. They are followed by cellPosErrorLambda1, cellPosErrorLambda2 and n_CellIdentities, which show the cell identity and error amounts, which are clearly effective factors in predicting the target class. Moderately important features include SINR, Longitude and Distance. This showed that signal quality and location information are still important for algorithm.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef3859-d4bf-4319-bf8b-eb8bd0d72b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explainer\n",
    "explainer = shap.LinearExplainer(log_model, X_train, feature_perturbation=\"interventional\")\n",
    "\n",
    "# calculate SHAP values \n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# summary plot: effect of the features\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad8ed6-627d-4345-9bf6-bcde0dbd776f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The plot given above indicates the SHAP value summary plot for the Logistic Regression model used in the binary classification task. This visualization provides both the magnitude and direction of the contribution of each feature to the model's predictions. To begin with, Power and RSRP are the most influential features and consistently affect the model output positively or negatively depending on their values. For instance, high Power values generally lead to a positive SHAP value indicating an increased probability of a particular class, while low values contribute negatively. Similarly, cellPosErrorLambda1, cellPosErrorLambda2, and n_CellIdentities also show a significant effect. This suggests that cell location error margins are important variables.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e42141-5b80-47be-bcfb-1668d6c3be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics\n",
    "rf_scores = [\n",
    "    accuracy_score(y_test, y_pred_rf),\n",
    "    precision_score(y_test, y_pred_rf, average='weighted'),\n",
    "    recall_score(y_test, y_pred_rf, average='weighted'),\n",
    "    f1_score(y_test, y_pred_rf, average='weighted')\n",
    "]\n",
    "\n",
    "lr_scores = [\n",
    "    accuracy_score(y_test, y_pred_lr),\n",
    "    precision_score(y_test, y_pred_lr, average='weighted'),\n",
    "    recall_score(y_test, y_pred_lr, average='weighted'),\n",
    "    f1_score(y_test, y_pred_lr, average='weighted')\n",
    "]\n",
    "\n",
    "# Labels and angles\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "num_vars = len(labels)\n",
    "\n",
    "# Close the plot by repeating the first value\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # close the radar chart\n",
    "\n",
    "rf_scores += [rf_scores[0]]\n",
    "lr_scores += [lr_scores[0]]\n",
    "\n",
    "# Create the radar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot each model\n",
    "ax.plot(angles, rf_scores, color='blue', linewidth=2, label='Random Forest')\n",
    "ax.fill(angles, rf_scores, color='blue', alpha=0.25)\n",
    "\n",
    "ax.plot(angles, lr_scores, color='green', linewidth=2, label='Logistic Regression')\n",
    "ax.fill(angles, lr_scores, color='green', alpha=0.25)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(\"Model Performance Comparison (Radar Chart)\", fontsize=15)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticks([0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "ax.set_ylim(0.6, 1.0)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.2, 1.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb74c5e-9b5f-4af3-9b81-6b0c684e6c51",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The radar chart presents a comparative visual of the models applied for binary classification. In the binary classification task, both Random Forest and Logistic Regression models showed strong performance, albeit with different strengths. Random Forest model achieved an accuracy of 0.89. In contrast, Logistic Regression achieved an accuracy of 0.85, offering better interpretability thanks to the coefficient-based feature importance. Feature analysis revealed that signal-related variables such as RSRP and Power were consistently significant in both models. However, Random Forest made more use of categorical features such as n_CellIdentities and Band. Overall, Random Forest was more accurate. Also, Logistic Regression provided clearer insights into feature effects.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d1edf-d74c-44c7-9550-c8e1bd7d3236",
   "metadata": {},
   "source": [
    "<a id=\"52-multi-class-classification\"></a> \n",
    "### 5.2. Multi-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311edb12-bcaa-4bb0-98d7-964cafd3b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[categorical_cols + numeric_cols].drop(columns=[\"scenario\",\"campaign\"]) # dropping target columns and the columns which showed high correlation with target column\n",
    "y = df[categorical_cols + numeric_cols][\"scenario\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62070165-a66d-4115-8d75-2022852a4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y,  shuffle=True) # train and test data splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd06d8c-c304-49b6-b694-15633a576c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train the model\n",
    "rf_model = RandomForestClassifier(\n",
    "    max_depth=5,             \n",
    "    random_state=42,\n",
    "    n_jobs=-1                 #\n",
    ")\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773b261-7494-44e4-8117-11b5abbcc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5a6f7-0251-4c17-ac1a-4fefcb39ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy and F1 score\n",
    "acc = accuracy_score(y_test, y_pred_rf)\n",
    "f1 = f1_score(y_test, y_pred_rf, average='macro')  # Using 'macro'  for multi-class\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9f400-8a32-4e98-b7db-0f649819d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf99ff-cedd-4f51-91a0-5cbbb16b01a8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The confusion matrix reveals that the model correctly predicted 39,268 class 0 instances, 14,491 class 1 instances, and 41,905 class 2 instances. However, the model misclassified 4,058 class 0 instances, 1,485 class 1 instances, and 3,467 class 2 instances. Besides, 829 class 1 instances were incorrectly classified as class 0, while 4 class 0 and 1 class 2 instance were predicted as class 1. These results show that the Random Forest model exhibits a strong ability to generalize across multiple classes with minimal misclassification. The classifier achieved an overall F1 Score of 0.91, further validating its robustness and reliability for multi-class prediction tasks.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51a0f8-fa77-493e-aa87-9ea264c35ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1a608-c3b8-4f45-a971-c8f06a3ad075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train.columns\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importances[indices][:15], y=features[indices][:15], palette='viridis')\n",
    "plt.title(\"Top 15 Feature Importances - Random Forest\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f15ce-0eb1-4ff7-adf9-a403ac79c716",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The figure given above indicates the top 15 features used by the Random Forest model. Accordingly, it shows that Speed is the most effective factor in predicting the scenario variable, followed by Longitude, Time and Latitude. These top features indicate that user movement patterns and location play a significant role in determining whether a scenario is classified as indoor, outdoor or other types. Temporal features such as day and time, together with contextual indicators such as cellLongitude and is_weekend, contribute significantly to the model's decision-making process. This insight highlights how both mobility and spatiotemporal factors are strongly related to the network usage context. It also highlights how the classifier effectively distinguishes between different user environments.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3268f09-b720-44c1-ac44-9c649df81a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis - Explain model predictions using SHAP values\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Check the shape of SHAP values\n",
    "print(\"SHAP Values Shape:\", np.array(shap_values).shape)\n",
    "\n",
    "if isinstance(shap_values, list):  # If SHAP values are returned as a list\n",
    "    shap_values_selected = shap_values[1]  # Take SHAP values for class \"1\"\n",
    "else:  \n",
    "    shap_values_selected = shap_values[:, :, 1]  # Extract SHAP values for class 1 from a multi-dimensional array\n",
    "\n",
    "# Verify the corrected shape\n",
    "print(\"Corrected SHAP Shape:\", shap_values_selected.shape)\n",
    "\n",
    "# If the shape does not match the expected format, reshape the values\n",
    "if shap_values_selected.shape[1] != X_test.shape[1]:  \n",
    "    shap_values_selected = shap_values_selected.reshape(X_test.shape)\n",
    "\n",
    "# Generate summary plot (Bar format) - Shows feature importance\n",
    "shap.summary_plot(shap_values_selected, X_test, plot_type=\"bar\")\n",
    "\n",
    "# Generate full summary plot - Shows feature impact on predictions\n",
    "shap.summary_plot(shap_values_selected, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6f5f6-6497-461a-88bc-040d345b44d1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The SHAP value plot provides a comprehensive interpretation of how individual features contribute to the classification of the Scenario variable. Features such as hour, speed, and latitude exhibit the highest SHAP values, indicating that they have a strong influence on the model's output. Higher speed values are associated with increased probability of outdoor scenarios, while lower values correspond to more static environments such as indoor scenario. Similarly, hour and day reflect the temporal dynamics that affect scenario classification, with certain time intervals showing distinct patterns of behaviour.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148031d-09c7-4b36-be7a-ddd9c53cb597",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classification (Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8052f3-ed3b-4952-8fd1-f72dc0d526f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "# Set random_state for reproducibility\n",
    "log_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96e48c-2f71-4014-bd9d-6c02e005647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_lr = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fe05c-dc95-4cc1-98e7-64a1f05fa0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy and F1 score\n",
    "acc = accuracy_score(y_test, y_pred_lr)\n",
    "f1 = f1_score(y_test, y_pred_lr, average='macro')  # Using 'macro' for multi-class\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))  # Precision, recall, F1-score per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ba102-3400-4626-a88a-725c09b453b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix visualization to analyze misclassifications\n",
    "cm = confusion_matrix(y_test, y_pred_lr)  # Generate confusion matrix\n",
    "plt.figure(figsize=(6, 4))  # Set figure size\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')  # Create heatmap\n",
    "plt.xlabel(\"Predicted\")  # Label x-axis\n",
    "plt.ylabel(\"Actual\")  # Label y-axis\n",
    "plt.title(\"Confusion Matrix\")  # Set plot title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059d3c3-52be-4a96-98ef-c0052da6465d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The confusion matrix presents the performance of the Logistic Regression model in a multi-class classification setting. The diagonal values (36,640 for class 0, 14,779 for class 1 and 36,308 for class 2) show the correctly classified examples for each class. Misclassifications can be seen in the off-diagonal cells with class 2 showing the highest confusion with class 0 (8,415 cases). As a result, the model shows successful performance with an F1 score of 0.84.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f28070-afa9-4087-859c-aedbb48af204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (Coefficient-Based)\n",
    "# - Logistic Regression coefficients represent the importance of features\n",
    "# - Higher absolute values indicate stronger influence on predictions\n",
    "coefficients = log_model.coef_[0]  # Extract coefficients (for binary classification)\n",
    "feature_names = X_train.columns  # Get feature names\n",
    "\n",
    "# Create a DataFrame to store feature importance\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,  # Feature names\n",
    "    'Coefficient': coefficients,  # Raw coefficients\n",
    "    'Importance': np.abs(coefficients)  # Absolute magnitude for importance ranking\n",
    "}).sort_values(by='Importance', ascending=False)  # Sort by importance\n",
    "\n",
    "# Visualizing feature importance using a bar plot\n",
    "plt.figure(figsize=(10, 6))  # Set figure size\n",
    "sns.barplot(x='Importance', y='Feature', data=coef_df, palette='coolwarm') \n",
    "plt.title(\"Feature Importance (Logistic Regression Coefficients)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Coefficient Magnitude (abs)\", fontsize=12) \n",
    "plt.ylabel(\"Feature\", fontsize=12)  \n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca1c32-3a6e-4913-9796-974b2bbd275e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The figure given above shows the feature importance derived from the absolute magnitude of the coefficients in the Logistic Regression model used for multi-class classification. In scenario classification, the feature that the model finds most influential is Speed, followed by hour and cellLatitude, indicating that temporal and mobility related features play an important role in classification decisions. Features like PCI, Frequency and cellIdentity contribute to model minimally.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca344e-5167-4a2c-9f17-a62312987d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KernelExplainer \n",
    "explainer = shap.Explainer(log_model, X_train, feature_names=X_train.columns)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# SHAP Summary Plot \n",
    "shap.summary_plot(shap_values, X_test, class_names=log_model.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ac5b2-109e-4847-969d-a9ac3ad1416d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "Figure given above presents the SHAP value plot for the Logistic Regression model applied in the multi-class classification task. This visualization reveals the average impact of each feature on the model’s output, distinguishing their importance across all three classes. Notably, hour, CellIdentity, and Speed stand out as the most impactful variables, with distinct contributions for each class label (0, 1, and 2). The color-coded bars indicate how these features influence predictions differently depending on the target class, offering a deeper interpretability compared to standard coefficient-based feature importance.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fa2a2-d137-4dae-9298-dced0f255735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics\n",
    "rf_scores = [\n",
    "    accuracy_score(y_test, y_pred_rf),\n",
    "    precision_score(y_test, y_pred_rf, average='weighted'),\n",
    "    recall_score(y_test, y_pred_rf, average='weighted'),\n",
    "    f1_score(y_test, y_pred_rf, average='weighted')\n",
    "]\n",
    "\n",
    "lr_scores = [\n",
    "    accuracy_score(y_test, y_pred_lr),\n",
    "    precision_score(y_test, y_pred_lr, average='weighted'),\n",
    "    recall_score(y_test, y_pred_lr, average='weighted'),\n",
    "    f1_score(y_test, y_pred_lr, average='weighted')\n",
    "]\n",
    "\n",
    "# Labels and angles\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "num_vars = len(labels)\n",
    "\n",
    "# Close the plot by repeating the first value\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # close the radar chart\n",
    "\n",
    "rf_scores += [rf_scores[0]]\n",
    "lr_scores += [lr_scores[0]]\n",
    "\n",
    "# Create the radar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot each model\n",
    "ax.plot(angles, rf_scores, color='blue', linewidth=2, label='Random Forest')\n",
    "ax.fill(angles, rf_scores, color='blue', alpha=0.25)\n",
    "\n",
    "ax.plot(angles, lr_scores, color='green', linewidth=2, label='Logistic Regression')\n",
    "ax.fill(angles, lr_scores, color='green', alpha=0.25)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(\"Model Performance Comparison (Radar Chart)\", fontsize=15)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticks([0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "ax.set_ylim(0.6, 1.0)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.2, 1.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c83b70-dc2f-4e19-9289-067539dfc0cf",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "Radar chart given above illustrates the performances of the models applied for multi classification as a radar chart. In this part, both Random Forest and Logistic Regression models showed strong performance. Random Forest model achieved 0.91 F1 score. In contrast, Logistic Regression achieved 0.84 F1 score. Random Forest achieved higher interpretability in terms of nonlinear relationships and feature interactions and effectively utilized mobility-related variables such as Speed, Longitude, and Time. Furthermore, SHAP analysis confirmed these insights by highlighting how dynamic features such as Time and Speed affected predictions across different scenario classes. Additionally, the Logistic Regression model provided clearer coefficient-based interpretability, although the F1 score was slightly lower. It also reaffirmed the importance of user movement and temporal behaviour by highlighting similar underlying variables such as Speed and Time. Overall, Random Forest provided better F1 score and enhanced decision logic.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d6c02-39b2-4d3f-a7bd-4baaaeef979e",
   "metadata": {},
   "source": [
    "<a id=\"6-optimization-using-genetic-algorithms-task-4\"></a> \n",
    "# 6. Optimization using Genetic Algorithms (Task 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85029c0d-9e5c-4d11-b2f3-cfafa89bacbb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "In this section, a Genetic Algorithm (GA) was implemented to optimize the hyperparameters of the Random Forest classifier used in the multi-class classification task. The main objective of using GA was to enhance the model’s predictive performance by exploring a broader combination of hyperparameters and selecting the most effective configuration through evolutionary principles (Elyan and Gaber, 2017).\n",
    "    </div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40966ce-98dd-4e29-94a3-653575706a07",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: justify;\">\n",
    "Genetic Algorithm was applied to optimize four basic hyperparameters of Random Forest classifier. These parameters are n_estimators, max_depth, min_samples_split and max_features. These parameters were chosen due to their known impact on the complexity, variance and performance of ensemble-based models.\n",
    "First, a population of ten individuals was created. Each individual represents a unique combination of hyperparameter values randomly sampled from predefined intervals. Furthermore, the fitness of each individual was evaluated using the weighted F1 score obtained through 3-fold cross-validation on the training set. This ensured that the optimization considered imbalances between classes and avoided overfitting to a particular fold (Wardhani et al., 2019). GA used tournament selection, where pairs of individuals were randomly sampled and the one with higher fitness was selected as the parent. A single-point crossover mechanism was then used to randomly inherit parameter values from both parents to produce offspring. To preserve diversity and avoid local optima, a mutation operation was applied with a fixed probability (20%) and one or more hyperparameters were modified by resampling from their ranges. Besides, elitism was implemented by directly moving the top two individuals with the highest fitness scores in each generation to the next generation. This step ensured that high-performing solutions were not lost (Tani et al., 2021).\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1a87e-5e1a-4e02-9208-fdb4a37f3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-score scorer for evaluation\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# STEP 1: Simplified parameter ranges\n",
    "param_ranges = {\n",
    "    'n_estimators': (50, 250),\n",
    "    'max_depth': (3, 6),\n",
    "    'min_samples_split': (2, 10),\n",
    "    'max_features': ['sqrt', 0.5]\n",
    "}\n",
    "\n",
    "# STEP 2: Fitness function\n",
    "def evaluate_f1(params, X, y):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        max_features=params.get('max_features', 'sqrt'),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    scores = cross_val_score(model, X, y, cv=3, scoring=f1_scorer)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# STEP 3: Initialize population\n",
    "def create_individual():\n",
    "    return {\n",
    "        'n_estimators': random.randint(*param_ranges['n_estimators']),\n",
    "        'max_depth': random.randint(*param_ranges['max_depth']),\n",
    "        'min_samples_split': random.randint(*param_ranges['min_samples_split']),\n",
    "        'max_features': random.choice(param_ranges['max_features'])\n",
    "    }\n",
    "\n",
    "population_size = 10\n",
    "population = [create_individual() for _ in range(population_size)]\n",
    "\n",
    "# STEP 4: Evaluate population\n",
    "def evaluate_population(pop, X, y):\n",
    "    return [evaluate_f1(ind, X, y) for ind in pop]\n",
    "\n",
    "# STEP 5: Genetic operators\n",
    "def select_parents(pop, scores, num_parents=3):\n",
    "    selected = []\n",
    "    for _ in range(num_parents):\n",
    "        candidates = random.sample(list(zip(pop, scores)), 2)\n",
    "        winner = max(candidates, key=lambda x: x[1])[0]\n",
    "        selected.append(winner)\n",
    "    return selected\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = {}\n",
    "    for param in param_ranges:\n",
    "        child[param] = random.choice([parent1[param], parent2[param]])\n",
    "    return child\n",
    "\n",
    "def mutate(individual, mutation_rate=0.2):\n",
    "    mutated = individual.copy()\n",
    "    for param in param_ranges:\n",
    "        if random.random() < mutation_rate:\n",
    "            if param == 'max_features':\n",
    "                mutated[param] = random.choice(param_ranges[param])\n",
    "            else:\n",
    "                mutated[param] = random.randint(*param_ranges[param])\n",
    "    return mutated\n",
    "\n",
    "# STEP 6: Evolution function \n",
    "def evolve(pop, scores, mutation_rate=0.2):\n",
    "    new_pop = []\n",
    "    elite_indices = np.argsort(scores)[-2:]\n",
    "    new_pop.extend([pop[i] for i in elite_indices])\n",
    "    \n",
    "    while len(new_pop) < len(pop):\n",
    "        parents = select_parents(pop, scores, 2)\n",
    "        child = crossover(parents[0], parents[1])\n",
    "        child = mutate(child, mutation_rate)\n",
    "        new_pop.append(child)\n",
    "    \n",
    "    return new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bf897-dda0-49de-b51e-c5c918ca873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Run evolution\n",
    "print(\"=== Initial Population ===\")\n",
    "fitness_scores = evaluate_population(population, X_train, y_train)\n",
    "for i, (ind, score) in enumerate(zip(population, fitness_scores)):\n",
    "    print(f\"Ind {i+1}: {ind} -> F1: {score:.4f}\")\n",
    "\n",
    "generations = 5\n",
    "best_scores = []\n",
    "\n",
    "for gen in range(1, generations+1):\n",
    "    population = evolve(population, fitness_scores)\n",
    "    fitness_scores = evaluate_population(population, X_train, y_train)\n",
    "    \n",
    "    best_idx = np.argmax(fitness_scores)\n",
    "    best_scores.append(fitness_scores[best_idx])\n",
    "    \n",
    "    print(f\"\\n=== Generation {gen} ===\")\n",
    "    print(f\"Best F1: {fitness_scores[best_idx]:.4f}\")\n",
    "    print(f\"Params: {population[best_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b38fb0-e094-4fc4-a88a-62d3acabdd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Final model\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, generations+1), best_scores, 'o-')\n",
    "plt.title('GA Optimization Progress (F1-score)')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Best F1-score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_individual = population[np.argmax(fitness_scores)]\n",
    "final_model = RandomForestClassifier(**best_individual, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n=== Final Model ===\")\n",
    "print(f\"Training F1: {f1_score(y_train, final_model.predict(X_train), average='weighted'):.4f}\")\n",
    "if 'X_test' in locals():\n",
    "    print(f\"Test F1: {f1_score(y_test, final_model.predict(X_test), average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445fdbf-f06b-4cb0-9ba1-271f112bbd54",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\r",
    "Over 5 generations, the population evolved towards higher fitness. The best F1 score increased from 0.946 in the first generation to 0.949 in the last generation. This demonstrated the effectiveness of GA in improving the parameter configuration of the model. Finally, the progression of the best scores over generations was visualized using a line graph to show convergence. Figure above illustrates the GA parameter optimization process.\n",
    "\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be35383-9b14-46a0-aa85-47dd1d0b8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics\n",
    "rf_scores = [\n",
    "    accuracy_score(y_test, y_pred_rf),\n",
    "    precision_score(y_test, y_pred_rf, average='weighted'),\n",
    "    recall_score(y_test, y_pred_rf, average='weighted'),\n",
    "    f1_score(y_test, y_pred_rf, average='weighted')\n",
    "]\n",
    "\n",
    "rf_ga_scores = [\n",
    "    accuracy_score(y_test, final_model.predict(X_test)),\n",
    "    precision_score(y_test, final_model.predict(X_test), average='weighted'),\n",
    "    recall_score(y_test, final_model.predict(X_test), average='weighted'),\n",
    "    f1_score(y_test, final_model.predict(X_test), average='weighted')\n",
    "]\n",
    "\n",
    "# Labels and angles\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "num_vars = len(labels)\n",
    "\n",
    "# Close the plot by repeating the first value\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # close the radar chart\n",
    "\n",
    "rf_scores += [rf_scores[0]]\n",
    "rf_ga_scores += [rf_ga_scores[0]]\n",
    "\n",
    "# Create the radar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot each model\n",
    "ax.plot(angles, rf_scores, color='blue', linewidth=2, label='Random Forest')\n",
    "ax.fill(angles, rf_scores, color='blue', alpha=0.25)\n",
    "\n",
    "ax.plot(angles, rf_ga_scores, color='green', linewidth=2, label='Random Forest (Optimized with GA)')\n",
    "ax.fill(angles, rf_ga_scores, color='green', alpha=0.25)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(\"Model Performance Comparison (Radar Chart)\", fontsize=15)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticks([0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "ax.set_ylim(0.6, 1.0)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.2, 1.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b22c7-3bdf-4581-92c5-a1c1a5bf583d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "The final optimized hyperparameters n_estimator 64, max_depth 6, min_samples_split 5 and max_features 0.5 were used to retrain a final Random Forest classifier on the training set. The optimized model achieved a test F1 score of 0.949, demonstrating strong generalization. It also outperformed the unoptimized model by improving its F1 score by around 0.04, compared to 0.91. The entire GA process emphasized interpretability and reproducibility while providing performance gains at relatively low computational cost. Furthermore, the figure above shows the multi classification performance of the optimized and unoptimized Random Forest models in the radar chart. To sum up, application of GA  for hyperparameter tuning in this study significantly improved the performance of the multi-class classification model. The results underscore the utility of evolutionary optimization techniques in finding optimal solutions in complex machine learning tasks.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b09b0-6d8a-4a21-9751-9182500e82bb",
   "metadata": {},
   "source": [
    "<a id=\"7-references\"></a> \n",
    "# 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0a283-6a81-4918-bafc-acf4909bfa43",
   "metadata": {},
   "source": [
    "- Hossain, T. and Inoue, S. (2019) 'A comparative study on missing data handling using machine learning for human activity recognition', 2019 Joint 8th International Conference on Informatics, Electronics & Vision (ICIEV) and 2019 3rd International Conference on Imaging, Vision & Pattern Recognition (icIVPR), Spokane, WA, USA, pp. 124-129. doi: 10.1109/ICIEV.2019.8858520.\n",
    "- Ayilara, O.F., Zhang, L., Sajobi, T.T., Sawatzky, R., Bohm, E., and Lix, L.M. (2019) 'Impact of missing data on bias and precision when estimating change in patient-reported outcomes from a clinical registry', Health Quality of Life Outcomes, 17(1), p.106. doi:  10.1186/s12955-019-1181-2.\n",
    "- Kousias, K., Rajiullah, M., Caso, G., Ali, U., Alay, Ö., Brunstrom, A., De Nardis, L., Neri, M. and Di Benedetto, M.-G. (2024) 'A large-scale dataset of 4G, NB-IoT, and 5G non-standalone network measurements', IEEE Communications Magazine, 62(5), pp. 44–49. doi: 10.1109/MCOM.011.2200707.\n",
    "- Forke, C. M. and Tropmann-Frick, M. (2021) ‘Feature engineering techniques and spatio-temporal data processing’, Datenbank Spektrum, 21, pp. 237–244. doi: 10.1007/s13222-021-00391-x.\n",
    "- Gil, P., Martins, H. and Januário, F. (2019) ‘Outliers detection methods in wireless sensor networks’, Artificial Intelligence Review, 52, pp. 2411–2436. doi: 10.1007/s10462-018-9618-2.\n",
    "- Rousseeuw, P. and Hubert, M. (2018) ‘Anomaly detection by robust statistics’, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8. doi: 10.1002/widm.1236.\n",
    "- Hubert, M. and Van der Veeken, S. (2008) ‘Outlier detection for skewed data’, Journal of Chemometrics, 22, pp. 235–246. doi: 10.1002/cem.1123. \n",
    "- CableFree (2025) ‘RSRP & RSRQ measurement in LTE’, CableFree, Available at: https://www.cablefree.net/wirelesstechnology/4glte/rsrp-rsrq-measurement-lte (Accessed: 01 April 2025).- \n",
    "- Topographic Map (2025) ‘Rome Topographic Map’, Topographic Map, Available at: https://en-gb.topographic-map.com/map-vbkn51/Rome/ (Accessed: 01 April 2025\n",
    "- Alshammari, A. (2024) ‘Implementation of feature selection using correlation matrix in Python’, International Journal of Computer Applications, 186, pp. 29–34. doi: 10.5120/ijca2024924341.\n",
    "- Skotarczak, E., Dobek, A. and Moliński, K. (2019) ‘Comparison of some correlation measures for continuous and categorical data’, Biometrical Letters, 56, pp. 253–261. doi: 10.2478/bile-2019-0015\n",
    "- Shi, C., Wei, B., Wang, W., Liu, H. and Liu, J. (2021) ‘A quantitative discriminant method of elbow point for the optimal number of clusters in clustering algorithm’, Journal of Wireless Communications and Networking, 2021(31). doi:10.1186/s13638-021-01910-w.-\n",
    "- Li, C., Günther, M., Dhamija, A.R., Cruz, S., Jafarzadeh, M., Ahmad, T. and Boult, T.E. (2022) ‘Agglomerative clustering with threshold optimization via extreme value theory’, Algorithms, 15(5), p. 170. doi:10.3390/a15050170.-\n",
    "- Lundberg, S.M. and Lee, S.-I. (2017) ‘A unified approach to interpreting model predictions’, Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 4768–4777. doi:10.5555/3295222.3295230.-\n",
    "- Elyan, E. and Gaber, M.M. (2017) ‘A genetic algorithm approach to optimising random forests applied to class engineered data’, Information Sciences, 384, pp. 220–234. doi:10.1016/j.ins.2016.08.007.\n",
    "- Wardhani, N., Rochayani, M., Iriany, A., Sulistyono, A. and Lestantyo, P. (2019) ‘Cross-validation metrics for evaluating classification performance on imbalanced data’, 2019 International Conference on Computer, Control, Informatics and its Applications (IC3INA), pp. 14–18. doi:10.1109/IC3INA48034.2019.8949568.- \n",
    "- Tani, L., Rand, D., Veelken, C. and Kadastik, M.  (2021) ‘Evolutionary algorithms for hyperparameter optimization in machine learning for application in high energy physics’, European Physical Journal C, 81(170). doi:10.1140/epjc/s10052-021-08950-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
